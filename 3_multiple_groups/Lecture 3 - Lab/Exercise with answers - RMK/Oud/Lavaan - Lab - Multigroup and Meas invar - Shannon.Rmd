---
title: "Lab Day 2b: Multigroup Models and Measurement Invariance"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

Today’s lab meeting consists of two exercises:

Exercise 1 addresses multigroup regression and, Exercise 2, measurement invariance.

Practical information:

* All the data and other files for these exercises can be found at the LLL platform. Make sure to unzip the files. The folder containing these files will be your working directory.

* Solutions to the exercises can be found in the Solutions folder. We provided R scripts for doing each of the exercises with `lavaan`.

### Exercise 1: Multigroup Regression

#### 1.1 In this exercise, we extend the regression analysis of Day 1 to a multi-group analysis (and thus use popular_regr.txt again and remember that -99 and -999 denote missings). In the multigroup analysis, you want to predict levels of socially desirable answering patterns of adolescents (sw) by overt (overt) and covert antisocial behaviour (covert), for males and females (gender).

**a. Create `lavaan` code for a multigroup comparison between males and females to answer the question whether there are differences in the regression coefficients between sw and overt.**

 * Bear in mind that gender is a grouping variable consisting of two levels. By making it a factor, one can easily assign labels (e.g., ‘males’ and ‘females’) to it.
 * To run a multigroup analysis (for gender), the `lavaan` function needs the argument: group = "gender".
 * To also obtain the confidence intervals use: ci = TRUE.
 * In this case, we will not use FIML, as we also started with in the regression analysis of Day 1.
 
You will now get results for males and females separately. 

*Are there differences between males and females?*

*What do the confidence intervals indicate?*

<details><summary><b>Click to show answers</b></summary>

**Get started:**

```{r, message=F, warning=F, eval=FALSE}
if (!require("lavaan")) install.packages("lavaan", dependencies = TRUE)
```

```{r}
library(lavaan) 
```

**Read in the data:**

```{r, message=F, warning=F}
data_regr <- read.table("popular_regr.txt", header = T)

#If data without header, then: 
colnames(data_regr) <- c("respnr", "Dutch", "gender", "sw", "covert", "overt")
```


**Denote missing values:**

Missing values are labelled -99 and -999, so set this to NA:

```{r, message=F, warning=F}
data_regr[sapply(data_regr, function(x) as.character(x) %in% c("-99", "-999") )] <- NA 
```

If you forget to tell R that -99 and -999 are used to denote missing data, these values will be treated as being observed. 

**Recoding variables:**

Besides sw, overt, and covert we now also use the variable gender. Since we read in a text file, there is no information about measurement level. Thus, we want to denote that the variable 'gender' is a factor, although it is not necessary in case of two levels. It is, however, helpful in assigning names to the levels.

```{r, message=F, warning=F}
data_regr$gender <- factor(data_regr$gender, labels = c("male", "female"))

#summary(data_regr$gender)
```

**Specify the multigroup regression model:**

Now, let us run a multigroup regression model predicting levels of socially desirable answering patterns of adolescents (sw) using the predictors overt (overt) and covert antisocial behaviour (covert), for both males and females.

To run a multigroup regression with `lavaan`, we need to add: group = "gender" to the `lavaan` function. It does not affect the model specification:

```{r, message=F, warning=F}
model.regression <- '
  sw ~ overt + covert # regression
  sw ~~ sw            # residual variance
  sw ~ 1              # intercept
'
```

**Fit the multigroup regression model using the `lavaan` function:**

```{r, message=F, warning=F}
fit_MGregr <- lavaan(model = model.regression, data = data_regr,
                     group = "gender") # multigroup specification
```

**Get a summary of the model:**

```{r, message=F, warning=F}
summary(fit_MGregr, standardized=TRUE, fit.measures = TRUE, rsquare = TRUE, ci = TRUE)
```

**Get the parameter estimates from the model fit:**

You can call for the parameter estimates in a few different ways, as outlined below. 

```{r, message=F, warning=F}
coef(fit_MGregr)                 # unstandardized regression parameters

parameterEstimates(fit_MGregr)   # all unstandardized parameters - with extra information

standardizedSolution(fit_MGregr) # all standardized parameters - with extra information
```

**Get the model fit measures:**

Again, you can call for the model fit measures in a few different ways. 

```{r, message=F, warning=F}
fitMeasures(fit_MGregr)   # many model fit measures

fitMeasures(fit_MGregr, c("cfi", "tli", "rmsea","srmr")) # ask for specific model fit measures
                        

inspect(fit_MGregr, 'r2') # R-square (for sw)

# Technical output
lavInspect(fit_MGregr) # Comparable to TECH1 in Mplus
```

**Possible conclusions:**

* The standardized effect of covert on sw appears to be stronger for females (-0.478) than males (-0.438) and reverse for the effect of overt (-0.161 for males compared to -0.112 for females). 
* However, the confidence intervals for sw on covert and overt show a large overlap for males and females. 
* Additionally, the regression coefficients for the males are included in the confidence intervals for the females and vice versa. 
* Therefore, we are below going to test whether the differences between the regression coefficients for males and females are statistically significant. 

**Plot the results:**

```{r, message=F, warning=F}
if (!require("lavaanPlot")) install.packages("lavaanPlot") # install this package first (once)
library(lavaanPlot)
lavaanPlot(model = fit_MGregr, node_options = list(shape = "box", fontname = "Helvetica"), 
           edge_options = list(color = "grey"), coefs = T, stand = T, covs = T) # plot

# Alternative
if (!require("tidySEM")) install.packages("tidySEM") # install this package first (once)
library(tidySEM)
graph_sem(fit_MGregr) # plot

# Another alternative
if (!require("semPlot")) install.packages("semPlot") # install this package first (once)
library(semPlot)
semPaths(fit_MGregr, "par", weighted = FALSE, nCharNodes = 7, shapeMan = "rectangle",
         sizeMan = 8, sizeMan2 = 5) # plot
```
 
</details>

<br>

**b. Compute a significance test (Wald test) to test whether the regression coefficients for males and females are the same or not.**

For a `lavaan` object (with label names!), one can do this with lavTestWald(fit, constraints = ‘<insert equality restriction>’). 

Now, there is only one equation for both groups, so one needs to assign two labels (say, b1_m and b1_f) to the ‘single’ parameter, by using: c(b1_m,b1_f)*

*What do you conclude about the equality of regression parameters (b1_m&b1_f and b2_m&b2_f)?*

<details><summary><b>Click to show answers</b></summary>

**Test equality of the regression coefficients:**

Next, we need to test whether the regression coefficients for males and females are the same.

Now, we want to constrain 'a single parameter' to be equal across two groups. Therefore, we need to label this parameter for both groups, so using 2 labels. This we need to do for both regression coefficient parameters.

**Specify the model:**

```{r, message=F, warning=F}
model.regression_equal <- '
  # model with labeled parameters
  sw ~ c(b1_m,b1_f)*overt + c(b2_m,b2_f)*covert # regression
  sw ~~ sw                                      # residual variance
  sw ~ 1                                        # intercept
'
```

**Fit the model:**

```{r, message=F, warning=F}
# Fit model
fit_MGregr_equal <- lavaan(model = model.regression_equal, data = data_regr,
                     group = "gender") # multigroup specification
```

**Conduct a Wald test:**

```{r, message=F, warning=F}
lavTestWald(fit_MGregr_equal, constraints = 'b1_m == b1_f; b2_m == b2_f')
```

The Wald test with df=1 is 1.035524, p = 0.5958526. Thus, we do not reject the null that the two regression coefficients for both groups are equal. Hence, we conclude that there is no evidence for a difference between b1_m and b1_f and between b2_m and b2_f.

In case one wants the model results for which the regression coefficients are the same:

**Specify the model:**

```{r, message=F, warning=F}
# Model specification
model.regression_equal <- '
  # model with labeled parameters
  sw ~ c(b1_m,b1_f)*overt + c(b2_m,b2_f)*covert # regression
  sw ~~ sw                                      # residual variance
  sw ~ 1                                        # intercept
  #
  # constraints
  b1_m == b1_f; b2_m == b2_f
'
```

**Fit  the model:**

```{r, message=F, warning=F}
fit_MGregr_equal <- lavaan(model = model.regression_equal, data = data_regr,
                           group = "gender") # multigroup specification 
```

**Get the model summary**

```{r}
summary(fit_MGregr_equal, standardized=TRUE, fit.measures=TRUE)

fitMeasures(fit_MGregr_equal, c("cfi", "tli", "rmsea","srmr")) # alternative for specific fit measures
```

</details>

<br>

### Exercise 2: Measurement Invariance

#### 2.1. In this exercise you will repeat the measurement invariance analyses of the lecture about Prolonged Grief Disorder. Use PGDdata2.txt; and note that missing data is denoted by -999. The data set consists of the grouping variable Kin2 (with two levels: partner and else) and 5 items taken from the Inventory of Complicated Grief (see the pdf of Boelen et al. (2010) in your zip-folder):

1. Yearning
2. Part of self died
3. Difficulty accepting the loss
4. Avoiding reminders of deceased
5. Bitterness about the loss 

**a. Run a 1-factor CFA on the data set ignoring the multigroup structure, using the default parameterization in the `cfa()` function.**

*How many subjects are there?*

*How about the fit of the model?*

*Which item has the weakest contribution to the latent factor (in terms of standardized factor loading and explained variance)?*

<details><summary><b>Click to show answers</b></summary>

**Read in the data:**
```{r, message=F, warning=F}
data_PGD <- read.table("PGDdata2.txt", header = T)
```

**Denote missing values:**
```{r, message=F, warning=F}
data_PGD[sapply(data_PGD, function(x) as.character(x) %in% c("-999") )] <- NA 

data_PGD$Kin2 <- factor(data_PGD$Kin2, labels = c("partner", "else"))

#summary(data_PGD$Kin2)
```

**Correlations for the items of interest: b1pss1 b2pss2 b3pss3 b4pss4 b5pss5**

```{r, message=F, warning=F}
if (!require("Hmisc")) install.packages("Hmisc", dependencies = TRUE)
library(Hmisc)

res <- rcorr(as.matrix(data_PGD[,2:6])) # rcorr() accepts matrices only
round(res$r, 3) # Correlation matrix (rounded to 3 decimals)
```

**One-Factor CFA:**

Specify and fit the model, then get the summary and fit measures:

```{r, message=F, warning=F}
# Specify:
model.1CFA <- "
 F  =~ b1pss1 + b2pss2 + b3pss3 + b4pss4 + b5pss5
"

# Fit:
fit_1CFA <- cfa(model.1CFA, data=data_PGD,
                missing='fiml', fixed.x=F) # Specify FIML 

# Summary:
summary(fit_1CFA, standardized = T)

# Fit statsitics:
fitMeasures(fit_1CFA, c("chisq", "df", "pvalue", "cfi", "tli", "rmsea","srmr")) 

lavInspect(fit_1CFA) # alternative
```

There number of observations is 571. Thus, there are 571 subjects.

Using the rules of thumb, we can conclude that the fit of the model is acceptable (RMSEA < .06, SRMR < .08, CFI/TLI > .95). 

Item 2 has the weakest contribution to the factor.Its standardized factor loading (Std.all) is 0.494. Hence, the item explains 100%*0.494^2 = approx 24.4% of the factor its variance.

</details>

<br>

**b. Run a 1-factor CFA on the data set with multigroup using: group = "Kin".**

*How many subjects are there per group?*

<details><summary><b>Click to show answers</b></summary>

**Multigroup one-factor CFA:**

Specify and fit the model, then get the summary and fit measures:

```{r, message=F, warning=F}
# Specify:
model.1CFA <- "
 F  =~ b1pss1 + b2pss2 + b3pss3 + b4pss4 + b5pss5
"

# Fit:
fit_MG1CFA <- cfa(model.1CFA, data=data_PGD,
                group = 'Kin2',
                missing='fiml', fixed.x=F) # Specify FIML 

# Summary:
summary(fit_MG1CFA, standardized = T)

# Fit statistics:
fitMeasures(fit_MG1CFA, c("chisq", "df", "pvalue", "cfi", "tli", "rmsea","srmr")) 

lavInspect(fit_MG1CFA) # alternative
```

There number of observations per group are:

* 190 in the partner group
* 379 other

</details>

<br>

**c. Test for configural, metric (weak) and scalar (strong) invariance.**

* Configural: model used in b:
* Weak: In the cfa function, one should add group.equal = "loadings".
* Strong: In the cfa function, one should add group.equal = c("intercepts", "loadings").

One can compare these model with: `lavTestLRT()`.

*What is the model fit in the configural, metric (weak) and scalar (strong) invariant model? *

*Which model do you prefer?*

<details><summary><b>Click to show answers</b></summary>

**Measurement invariance testing:**

Before one compares parameters across multiple groups, one first needs to establish measurement invariance. When data is continuous, testing for measurement invariance involves a fixed sequence of model comparison tests. A typical sequence involves three models:

Model 1: configural invariance. 
* The same factor structure is imposed on all groups.

```{r, message=F, warning=F}
# configural invariance - the same as fit_MG1CFA
fit_MG1CFA_ci <- cfa(model.1CFA, data=data_PGD,
                  group = 'Kin2',
                  missing='fiml', fixed.x=F)
```

Model 2: metric/weak invariance. 
* The factor loadings are constrained to be equal across groups.

```{r, message=F, warning=F}
# metric/weak invariance
fit_MG1CFA_wi <- cfa(model.1CFA, data=data_PGD,
                     group = 'Kin2',
                     missing='fiml', fixed.x=F,
                      group.equal = "loadings") 
```

Model 3: scalar/strong invariance. 
* The factor loadings and intercepts are constrained to be equal across groups.

```{r, message=F, warning=F}
# scalar/strong invariance
fit_MG1CFA_si <- cfa(model.1CFA, data=data_PGD,
                     group = 'Kin2',
                     missing='fiml', fixed.x=F,
                      group.equal = c("intercepts", "loadings"))
```

To compare the models the following code can be run:

```{r, message=F, warning=F}
lavTestLRT(fit_MG1CFA_ci, fit_MG1CFA_wi, fit_MG1CFA_si)
```

Because we provided three model fits, it will produce two tests: 

1. the first model versus the second model. 
2. the second model versus the third model. 

Chi-Squared Difference Test

Model         |Df |  AIC  | BIC    | Chisq | Chisq diff | Df diff | Pr(>Chisq)  
--------------|:-:|:-----:|:------:|:-----:|:----------:|:-------:|:-----------
fit_MG1CFA_ci  10  6488.1  6618.4  11.329                                
fit_MG1CFA_wi  14  6488.2  6601.1  19.417  8.0876        4         0.08842 .
fit_MG1CFA_si  18  6485.1  6580.6  24.297  4.8802        4         0.29982  
------------------------------------------------------------------------------|
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
------------------------------------------------------------------------------|

**Possible Conclusions:**

The configural model fits the data (chi-square = 11.329, p = .33 ):

```{r, message=F, warning=F}
fitMeasures(fit_MG1CFA_ci, c("chisq", "df", "pvalue", "cfi", "tli", "rmsea","srmr"))
```

From there on, we evaluate whether the more constrained model does not fit the data worse than the less constrained model:

Because the first p-value is non-significant, we conclude that:

* weak invariance (equal factor loadings) is supported in this dataset.
* Stated otherwise: The metric invariance model does not fit worse than the configural model.
* Because the second p-value is also non-significant, we conclude that strong invariance (equal intercepts and factor loadings) is also supported.
* In SEM, one always prefer parsimony: the model with the most df. Thus, we prefer the scalar invariance model, where both factor loadings and intercepts are constrained (to be equal). 
* Since the latent variable in the scalar model means the same thing across the two groups, we can compare values on the latent variable.

*Note:* Let us say that the second p-value is significant.

* Then, strong invariance is not supported. 
* Then, it is unwise to directly compare parameters across the two groups.

**Plot:**

Plot the results. 

```{r, message=F, warning=F}
if (!require("tidySEM")) install.packages("tidySEM") # install this package first (once)
library(tidySEM)

graph_sem(fit_MG1CFA_si) # plot
```

</details>

<br>

**d. Add constraints such that also the residual variances are constrained to be the same for both groups. In the cfa function in the group.equal argument, one should add "residuals".**

Check for AIC and BIC differences and use the Chi-square difference test, by using the `anova()` function.

*Did the model get significantly worse? *

<details><summary><b>Click to show answers</b></summary>

**Adding constraints:**

Add constraints such that also the residual variances are constrained to be the same for both groups: 

```{r, message=F, warning=F}
# Fit model:
fit_MG1CFA_si_res <- cfa(model.1CFA, data=data_PGD,
                     group = 'Kin2',
                     missing='fiml', fixed.x=F,
                     group.equal = c("intercepts", "loadings", "residuals")) # constraints
#group.equal = c("residual.covariances")

# Fit measures
fitMeasures(fit_MG1CFA_si_res, c("chisq", "df", "pvalue", "cfi", "tli", "rmsea","srmr"))

lavInspect(fit_MG1CFA_si_res) # alternative
```

Compare this model (fit_MG1CFA_si_res) to one without (fit_MG1CFA_si):

```{r, message=F, warning=F}
anova(fit_MG1CFA_si_res, fit_MG1CFA_si)
```

Chi-Squared Difference Test

Model             |Df   |  AIC    | BIC      | Chisq        | Chisq diff | Df diff    | Pr(>Chisq)|  
------------------|:----|:--------|:---------|:-------------|:-----------|:-----------|:----------|
fit_MG1CFA_si     | 18  | 6485.1  |  6580.6  |  24.297      |            |            |                              
fit_MG1CFA_si_res | 23  | 6485.3  |  6559.2  |  34.565      | 10.268     |   5        |  0.06799 

Signif. codes: 0 ‘ *** ’  0.001 ‘ ** ’ 0.01 ‘ * ’  0.05 ‘.’ 0.1 ‘ ’  1                    


**Possible conclusion:**
* Delta chi-square (5) = 10.268, p= 0.068.
* Hence, the model is not significantly worse. That is, The residual variances are also equal. 
* Bear in mind that we always prefer a more parsimonious model (= more df, that is, less parameters to be estimated) when we compare two models. 

</details>

<br>
