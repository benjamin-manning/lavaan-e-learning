%%% Title:    IDS Lecture 4
%%% Author:   Kyle M. Lang
%%% Created:  2017-09-12
%%% Modified: 2020-04-20

\documentclass{beamer}
\usetheme[%
  pageofpages          = of,
  bullet               = circle,
  titleline            = true,
  alternativetitlepage = true,
  titlepagelogo        = Logo3,
  watermark            = watermarkTiU,
  watermarkheight      = 100px,
  watermarkheightmult  = 4%
]{UVT}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{booktabs}
\usepackage{fancybox}

\newcommand{\kfold}[0]{\emph{K}-fold cross-validation}

%% Ensure styles of `blocks' (used in Definitions, Theorems etc.) follows the
%% UVT-style theme:
\setbeamercolor{block title}{fg = darkblue, bg = white}
\setbeamercolor{block body}{use = block title, bg = block title.bg}

%% Ensure TableOfContents is in UVT-style theme:
\setbeamercolor{section in toc}{fg = darkblue}

\title{The Generalized Linear Model \& Logistic Regression}
\subtitle{Introduction to Data Science Lecture 4}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Tilburg University}
\date{Block 4 2020}

\begin{document}

<<setup, include=FALSE>>=
set.seed(235711)

library(knitr)
library(ggplot2)
library(MASS)
library(mvtnorm)
library(glmnet)
library(xtable)
library(nnet)
library(MLmetrics)

source("../../../code/supportFunctions.R")
dataDir <- "../data/"

opts_chunk$set(size = 'footnotesize', fig.align = 'center')
knit_theme$set('edit-kwrite')

lightBlue <- rgb(0, 137, 191, max = 255)
midBlue   <- rgb(0, 131, 183, max = 255)
darkBlue  <- rgb(0, 128, 179, max = 255)
deepGold  <- rgb(184, 138, 45, max = 255)
lightGold <- rgb(195, 146, 48, max = 255)
@

%------------------------------------------------------------------------------%

\begin{frame}[t,plain]
\titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  
  \begin{enumerate}
  \item Generalized linear models
    \vb
  \item Logistic regression
    \vb
  \item Multinomial logistic regression
    \vb
  \item Classification via logistic regression
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{General Linear Model}
  
  So far, we've been discussing models with this form:
  \begin{align*}
    Y = \beta_0 + \sum_{p = 1}^P \beta_p X_p + \varepsilon
  \end{align*}
  This type of model is known as the \emph{general linear model}.
  \vc
  \begin{itemize}
  \item All flavors of linear regression are general linear models.
    \vc
    \begin{itemize}
    \item ANOVA
    \item ANCOVA
    \item Multilevel linear regression models
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Components of the General Linear Model}
 
  We can break our model into pieces:
  \begin{align*}
    \eta &= \beta_0 + \sum_{p = 1}^P \beta_p X_p\\
    Y &= \eta + \varepsilon
  \end{align*}
  Because $\varepsilon \sim \text{N}(0, \sigma^2)$, we can also write:
  \begin{align*}
    Y \sim \text{N}(\eta, \sigma^2)
  \end{align*}
  In this representation:
  \begin{itemize}
  \item $\eta$ is the \emph{systematic component} of the model
  \item The normal distribution, $\text{N}(\cdot, \cdot)$, is the model's 
    \emph{random component}.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Components of the General Linear Model}
  
  The purpose of general linear modeling (i.e., regression modeling) is to build
  a model of the outcome's mean, $\mu_Y$.  
  \begin{itemize}
  \item In this case, $\mu_Y = \eta$.
  \item The systematic component defines the mean of $Y$.
  \end{itemize}
  \vb
  The random component quantifies variability (i.e., error variance) around
  $\mu_Y$.  
  \begin{itemize}
  \item In the general linear model, we assume that this error variance follows
    a normal distribution.
  \item Hence the normal random component.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Generalized Linear Model}
  
  We can generalize the models we've been using in two important ways:
  \vc
  \begin{enumerate}
  \item Allow for random components other than the normal distribution.
    \vc
  \item Allow for more complicated relations between $\mu_Y$ and $\eta$.
    \begin{itemize}
    \item Allow: $g(\mu_Y) = \eta$
    \end{itemize}
  \end{enumerate}
  \vc
  These extensions lead to the class of \emph{generalized linear models} (GLMs).
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Components of the Generalized Linear Model}
  
  The random component in a GLM can be any distribution from the so-called 
  \emph{exponential family}.
  \vc
  \begin{itemize}
  \item The exponential family contains many popular distributions:
    \vc
    \begin{itemize}
    \item Normal
    \item Binomial
    \item Poisson
    \item Many others...
    \end{itemize}
  \end{itemize}
  \vc
  The systematic component of a GLM is exactly the same as it is in general 
  linear models:
  \begin{align*}
    \eta = \beta_0 + \sum_{p = 1}^P \beta_p X_p
  \end{align*}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Link Functions}
  
  In GLMs, $\eta$ does not directly describe $\mu_Y$.
  \begin{itemize}
  \item We first transform $\mu_Y$ via a \emph{link function}.
  \item $g(\mu_Y) = \eta$
  \end{itemize}
  \vb
  The link function allows GLMs for outcomes with restricted ranges without 
  requiring any restrictions on the range of the $\{X_p\}$.
  \vc
  \begin{itemize}
  \item For strictly positive $Y$, we can use a \emph{log link}: 
    \begin{align*}
      \ln(\mu_y) = \eta.
    \end{align*}
  \item The general linear model employs the \emph{identity link}: 
    \begin{align*}
      \mu_y = \eta.
    \end{align*}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Components of the Generalized Linear Model}
  
  Every GLM is built from three components:
  \vb
  \begin{enumerate}
  \item The systematic component, $\eta$.
    \begin{itemize}
    \item A linear function of the predictors, $\{X_p\}$.
    \item Describes the association between $\mathbf{X}$ and $Y$.
    \end{itemize}
    \vb
  \item The link function, $g(\mu_Y)$.
    \begin{itemize}
    \item Transforms $\mu_Y$ so that it can take any value on the real line.
    \end{itemize}
    \vb
  \item The random component, $P(Y|g^{-1}(\eta))$
    \begin{itemize}
    \item The distribution of the observed $Y$.
    \item Quantifies the error variance around $\eta$.
    \end{itemize}
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{General Linear Model $\subset$ Generalized Linear Model}
  
  The general linear model is a special case of GLM.
  \vb  
  \begin{enumerate}
  \item Systematic component:
    \begin{align*}
      \eta = \beta_0 + \sum_{p = 1}^P \beta_p X_p
    \end{align*}
  \item Link function:
    \begin{align*}
      \mu_Y = \eta
    \end{align*}
  \item Random component:
    \begin{align*}
      Y \sim \text{N}(\eta, \sigma^2)
    \end{align*}
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{}
  
  \begin{center}
    \Huge{\textsc{Logistic Regression}}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Logistic Regression}
  
  So why do we care about the GLM when linear regression models have worked
  thus far?  
  \begin{itemize}
  \item In a word: Classification.
  \end{itemize}
  \vb
  In the classification task, we have a discrete, qualitative outcome.
  \begin{itemize}
  \item We will begin with the situation of two-level outcomes.
    \begin{itemize}
    \item Alive or Dead
    \item Pass or Fail
    \item Pay or Default
    \end{itemize}
  \end{itemize}
  \vb
  We want to build a model that predicts class membership based on some set of 
  interesting features.
  \begin{itemize}
  \item To do so, we will use a very useful type of GLM: \emph{logistic 
    regression}.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Classification Example}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      Suppose we want to know the effect of study time on the probability of
      passing an exam.  
      \vc
      \begin{itemize}
      \item The probability of passing must be between 0 and 1.
        \vc
      \item We care about the probability of passing, but we only observe 
        absolute success or failure.
        \vc
        \begin{itemize}
        \item $Y \in \{1, 0\}$
        \end{itemize}
      \end{itemize}
      
      \end{column}
    
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
beta <- c(-5.5, 1)
x    <- runif(100, 0, 12)
eta  <- beta[1] + beta[2] * x
pi   <- exp(eta) / (1 + exp(eta))
y    <- rbinom(100, 1, pi)

dat1 <- data.frame(x, y)

p1 <- gg0(x = dat1$x, y = dat1$y) +
    xlab("Hours of Study") +
    ylab("Probability of Passing")
p1
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Linear Regression for Binary Outcomes?}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      What happens if we try to model these data with linear regression?  
      \vc
      \begin{itemize}
      \item Hmm...notice any problems?
      \end{itemize}
      
    \end{column}
    
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
p1 + geom_smooth(method = "lm", se = FALSE)
@ 

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Logistic Regression Visualized}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      We get a much better model using logistic regression.
      \vc
      \begin{itemize}
      \item The link function ensures legal predicted values.
        \vc
      \item The sigmoidal curve implies fluctuation in the effectiveness of
        extra study time.  
        \vc
        \begin{itemize}
        \item More study time is most beneficial for students with around 
          \Sexpr{round(-beta[1] / beta[2], 2)} hours of study. 
        \end{itemize}
      \end{itemize}
      
    \end{column}
    
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
tmp  <- with(dat1, seq(min(x), max(x), length.out = 500))
eta2 <- beta[1] + beta[2] * tmp
pi2  <- exp(eta2) / (1 + exp(eta2))
dat2 <- data.frame(x = tmp, y = pi2)

p1 + geom_line(aes(x = x, y = y), data = dat2, colour = "blue", size = 1)
@ 

\end{column}
\end{columns}
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Defining the Logistic Regression Model}
  
  In logistic regression problems, we are modeling binary data:
  \begin{itemize}
  \item Usual coding: $Y \in \{1 = \text{``Success''}, 0 = \text{``Failure''} \}$.
  \end{itemize}
  \vb
  The \emph{Binomial} distribution is a good way to represent this kind of data.
  \begin{itemize}
  \item The systematic component in our logistic regression model will be the 
    binomial distribution.
  \end{itemize}
  \vb
  The mean of the binomial distribution (with $N = 1$) is the ``success'' 
  probability, $\pi = P(Y = 1)$.
  \begin{itemize}
  \item We are interested in modeling $\mu_Y = \pi$:
    \begin{align*}
      g(\pi) = \beta_0 + \sum_{p = 1}^P \beta_p X_p
    \end{align*}
  \end{itemize}
  
  \comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BEGIN \comment{}
  \vc
  \begin{itemize}
  \item The \emph{Bernoulli} distribution is the most natural representation of 
    this type of data.
    \begin{align*}
      \text{Bern}(Y\text{;}~\pi) = \pi^Y(1 - \pi)^{1 - Y}
    \end{align*}
  \item In logistic regression, we work with a generalization of the 
    \emph{Bernoulli} distribution, namely the \emph{Binomial} distribution:
    \begin{align*}
      \text{Bin}(K\text{;}~\pi, N) = \binom{N}{K} \pi^K(1 - \pi)^{N - K}
    \end{align*}
  %\item Note that $Bin(\pi, 1) = Bern(\pi)$.
  \end{itemize}
  }%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END \comment{}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Link Function for Logistic Regression}
  
  Because $\pi$ is bounded by 0 and 1, we cannot model it directly---we must 
  apply an appropriate link function.
  \vc
  \begin{itemize}
  \item Logistic regression uses the \emph{logit link}.
    \vc
    %\begin{itemize}
  \item Given $\pi$, we can define the \emph{odds} of success as:
    \begin{align*}
      O_s = \frac{\pi}{1 - \pi}
    \end{align*}
  \item Because $\pi \in [0, 1]$, we know that $O_s \geq 0$.
    \vc
  \item We take the natural log of the odds as the last step to fully map 
    $\pi$ to the real line.
    \begin{align*}
      \text{logit}(\pi) = \ln \left(\frac{\pi}{1 - \pi}\right)
    \end{align*}
    %\end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Fully Specified Logistic Regression Model}
  
  Our final logistic regression model is:
  \begin{align*}
    Y &\sim \text{Bin}(\pi, 1)\\
    \text{logit}(\pi) &= \beta_0 + \sum_{p = 1}^P \beta_p X_p
  \end{align*}
  The fitted model can be represented as:
  \begin{align*}
    \text{logit}(\hat{\pi}) = \hat{\beta}_0 + \sum_{p = 1}^P \hat{\beta}_p X_p
  \end{align*}
  The fitted coefficients, $\{\hat{\beta}_0, \hat{\beta}_p\}$, are interpreted 
  in units of \emph{log odds}.

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Logistic Regression Example}
  
<<echo = FALSE>>=
out1 <- glm(y ~ x, data = dat1, family = binomial())

b0 <- round(coef(out1)[1], 3)
b1 <- round(coef(out1)[2], 3)

b0e <- round(exp(coef(out1))[1], 3)
b1e <- round(exp(coef(out1))[2], 3)
@ 

If we fit a logistic regression model to the test-passing data plotted above, we 
get:
\begin{align*}
  \text{logit}(\hat{\pi}_{pass}) = \Sexpr{b0} + \Sexpr{b1} X_{study}
\end{align*}
\vx{-12}
\begin{itemize}
\item A student who does not study at all has \Sexpr{b0} log odds of passing the
  exam.  
\item For each additional hour of study, a student's log odds of passing increase
  by \Sexpr{b1} units.
\end{itemize}
\vb
Log odds do not lend themselves to interpretation.
\begin{itemize}
\item We can convert the effects back to an odds scale by exponentiation.
  %\begin{itemize}
\item $\hat{\beta}$ has log odds units, but $e^{\hat{\beta}}$ has odds units.
  %\end{itemize}
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Interpretations} 
 
  Exponentiating the coefficients also converts the additive effects to 
  multiplicative effects.
  \vc
  \begin{itemize}
  \item $\ln(AB) = \ln(A) + \ln(B)$
  \item We can interpret $\hat{\beta}$ as we would in linear regression:
    \begin{itemize}
    \item A unit change in $X_p$ produces an expected change of $\hat{\beta}_p$ 
      units in $\text{logit}(\pi)$.
    \end{itemize}
    \vc
  \item After exponentiation, however, unit changes in $X_p$ imply multiplicative 
    changes in $O_s = \pi / (1 - \pi)$.
    \begin{itemize}
    \item A unit change in $X_p$ results in multiplying $O_s$  by 
      $e^{\hat{\beta}_p}$.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Interpretations}
  
  Exponentiating the coefficients in our toy test-passing example produces the 
  following interpretations:
  \begin{itemize}
  \item A student who does not study is expected to pass the exam with odds of 
    \Sexpr{b0e}.
  \item For each additional hour a student studies, their odds of passing
    increase by \Sexpr{b1e} \emph{times}.
    \begin{itemize}
    \item Odds of passing are \emph{multiplied} by \Sexpr{b1e} for each extra 
      hour of study.
    \end{itemize}
  \end{itemize}
    
  \vb
  \pause
  
  Due to the confusing interpretations of the coefficients, we often focus
  on the valance of the effects: 
  \begin{itemize}
  \item Additional study time is associated with increased odds of passing.
  \item $\hat{\beta_p} > 0$ = ``Increased Success'',  $e^{\hat{\beta}_p} > 1$ = 
    ``Increased Success''
  \end{itemize}
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Multiple Logistic Regression}
  
  The preceding example was a \emph{simple logistic regression}.
  \vc
  \begin{itemize}
  \item Including multiple predictor variables in the systematic component leads 
    to \emph{multiple logistic regression}.
    \vc
  \item The relative differences between simple logistic regression and multiple
    logistic regression are the same as those between simple linear regression
    and multiple linear regression.
    \vc
    \begin{itemize}
    \item The only important complication is that the regression coefficients 
      become partial effects.
    \end{itemize}
  \end{itemize}
  
\end{frame}
  
%------------------------------------------------------------------------------%

\begin{frame}{Multiple Logistic Regression Example}
  
<<echo = FALSE>>=
diabetes <- readRDS(paste0(dataDir, "diabetes.rds"))

diabetes$highGlu <- as.numeric(diabetes$glu > 100)

diabetes$age40 <- diabetes$age - 40
diabetes$bmi25 <- diabetes$bmi - 25
diabetes$bp100 <- diabetes$bp - 100

out1 <-
    glm(highGlu ~ age40 + bmi25 + bp100, data = diabetes, family = binomial())

beta1  <- coef(out1)
eBeta1 <- exp(beta1)

b0 <- round(beta1[1], 3)
b1 <- round(beta1[2], 3)
b2 <- round(beta1[3], 3)
b3 <- round(beta1[4], 3)

eB0 <- round(eBeta1[1], 3)
eB1 <- round(eBeta1[2], 3)
eB2 <- round(eBeta1[3], 3)
eB3 <- round(eBeta1[4], 3)
@ 

Suppose we want to predict the probability of a patient having ``high'' blood 
glucose from their age, BMI, and average blood pressure.
\vc
\begin{itemize}
\item We could do so with the following model:
  \begin{align*}
    \text{logit}(\pi_{hi.gluc}) = 
    \beta_0 + \beta_1 X_{age.40} + \beta_2 X_{BMI.25} + \beta_3 X_{BP.100}
  \end{align*}
\item By fitting this model to our usual ``diabetes'' data we get:
  \begin{align*}
    \text{logit}(\hat{\pi}_{hi.gluc}) = 
    \Sexpr{b0} + \Sexpr{b1} X_{age.40} + \Sexpr{b2} X_{BMI.25} + \Sexpr{b3} X_{BP.100}
  \end{align*}
\item Exponentiating the coefficients produces:
  \begin{align*}
    \frac{\hat{\pi}_{hi.gluc}}{1 - \hat{\pi}_{hi.gluc}} = 
    \Sexpr{eB0} \times \Sexpr{eB1}^{X_{age.40}} \times \Sexpr{eB2}^{X_{BMI.25}} \times 
    \Sexpr{eB3}^{X_{BP.100}}
  \end{align*}
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%
  
\begin{frame}{Exponentiating the Systematic Component}
  
  \begin{align*}
    \text{logit}(\hat{\pi}_{hi.gluc}) &= \Sexpr{b0} + \Sexpr{b1} X_{age.40} + 
    \Sexpr{b2} X_{BMI.25} + \Sexpr{b3} X_{BP.100}\\[12pt]
    e^{\text{logit}(\hat{\pi}_{hi.gluc})} &= 
    e^{\left(\Sexpr{b0} ~ + ~ \Sexpr{b1} X_{age.40} ~ + ~ \Sexpr{b2} X_{BMI.25} 
      ~ + ~ \Sexpr{b3} X_{BP.100} \right)}\\[8pt]
    \frac{\hat{\pi}_{hi.gluc}}{1 - \hat{\pi}_{hi.gluc}} &= 
    e^{\Sexpr{b0}} \times e^{\Sexpr{b1} X_{age.40}} \times e^{\Sexpr{b2} X_{BMI.25}} \times 
    e^{\Sexpr{b3} X_{BP.100}}\\[8pt]
    &= \left(e^{\Sexpr{b0}}\right) \times \left(e^{\Sexpr{b1}}\right)^{X_{age.40}} \times 
    \left(e^{\Sexpr{b2}}\right)^{X_{BMI.25}} \times 
    \left(e^{\Sexpr{b3}}\right)^{X_{BP.100}}\\[14pt]
    &= \Sexpr{eB0} \times \Sexpr{eB1}^{X_{age.40}} \times \Sexpr{eB2}^{X_{BMI.25}} 
    \times \Sexpr{eB3}^{X_{BP.100}}
  \end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{}
  
  \begin{center}
    \Huge{\textsc{Multinomial Logistic Regression}}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Multi-Class Outcomes}
  
  So, what do we do if our outcome takes more than two levels?
  \begin{itemize}
    \item Voting intention = \{Will vote, Won't vote, Not sure\}
    \item Preferred caffeine source = \{Coffee, Tea, Energy drink, None\}
    \item Current mood = \{Happy, Sad, Angry, Neutral\}
  \end{itemize}
  \vb
  \pause
  We saw that using a nominal variable with $L$ response levels as a predictor 
  requires creating $L - 1$ dummy codes.
  \begin{itemize}
  \item We could solve our problem by estimating $L - 1$ separate logistic 
    regression models.
  \item Do you see any problems with that approach?
  \end{itemize}
  \vb
  \pause
  We have a better way: \emph{Multinomial logistic regression}.
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Defining the Multinomial Logistic Regression Model}
  
  In multinomial logistic regression problems, we are modeling multi-class 
  nominal data:
  \begin{itemize}
  \item Usual coding: $Y \in \{1, 2, \ldots, L\}$.
  \end{itemize}
  \vb
  The \emph{Multinomial} distribution---a generalization of the binomial 
  distribution---is a good way to represent this kind of data.
  \begin{itemize}
  \item The systematic component in our multinomial logistic regression model 
    will be the multinomial distribution.
  \end{itemize}
  \vb
  We are interested in modeling the $L - 1$ probabilities, $\pi_l = P(Y = l)$, 
  of endorsing each response level instead of the \emph{baseline} level.
  \begin{align*}
    g(\pi_l) = \beta_{l0} + \sum_{p = 1}^P \beta_{lp} X_p, ~~ l = 2, 3, \ldots, L
  \end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Full Multinomial Logistic Regression Model}
  
  Given $L$ unique response levels for $Y$, our final multinomial logistic 
  regression model is:
  \begin{align*}
    Y &\sim \text{Multinom}(\Pi, \mathbf{1}), ~~ \Pi = \{\pi_2, \pi_3, \ldots, 
    \pi_L\}\\
    \text{logit}(\pi_l) &= \beta_{l0} + \sum_{p = 1}^P \beta_{lp} X_p, ~~ l = 2, 3,
    \ldots, L
  \end{align*}
  The fitted model can be represented as:
  \begin{align*}
    \text{logit}(\hat{\pi}_l) = \hat{\beta}_{l0} + \sum_{p = 1}^P \hat{\beta}_{lp} 
    X_p, ~~ l = 2, 3, \ldots, L
  \end{align*}
  Note that we, \emph{simultaneously}, estimate $L - 1$ separate sets of 
  coefficients, $\{\beta_{l0}, \beta_{lp}\}$.   
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks, fragile]{Example}

<<echo = FALSE>>=
glu3                                          <- rep(1, nrow(diabetes))
glu3[diabetes$glu > 80 & diabetes$glu <= 100] <- 2
glu3[diabetes$glu > 100]                      <- 3

diabetes$glu3 <- factor(glu3, levels = 1 : 3, labels = c("low", "mid", "hi"))

out2   <- multinom(glu3 ~ age40 + bmi25 + bp100, data = diabetes, trace = FALSE)
beta2  <- coef(out2)
eBeta2 <- exp(beta2)

b20 <- round(beta2[1, 1], 3)
b21 <- round(beta2[1, 2], 3)
b22 <- round(beta2[1, 3], 3)
b23 <- round(beta2[1, 4], 3)

b30 <- round(beta2[2, 1], 3)
b31 <- round(beta2[2, 2], 3)
b32 <- round(beta2[2, 3], 3)
b33 <- round(beta2[2, 4], 3)

eB20 <- round(eBeta2[1, 1], 3)
eB21 <- round(eBeta2[1, 2], 3)
eB22 <- round(eBeta2[1, 3], 3)
eB23 <- round(eBeta2[1, 4], 3)

eB30 <- round(eBeta2[2, 1], 3)
eB31 <- round(eBeta2[2, 2], 3)
eB32 <- round(eBeta2[2, 3], 3)
eB33 <- round(eBeta2[2, 4], 3)
@ 

Suppose we want to predict the probability of a patient having ``high'' or 
``moderate'' blood glucose, versus ``low'' blood glucose, from their age, BMI, 
and average blood pressure.
\vc
\begin{itemize}
\item We could do so with the following model:
  \begin{align*}
    \text{logit}(\pi_l) = 
    \beta_{l0} + \beta_{l1} X_{age.40} + \beta_{l2} X_{BMI.25} + \beta_{l3} X_{BP.100}
  \end{align*}
\item By fitting this model to our usual ``diabetes'' data we get:
  \begin{align*}
    \text{logit}(\hat{\pi}_{hi.gluc}) &= 
    \Sexpr{b30} + \Sexpr{b31} X_{age.40} + \Sexpr{b32} X_{BMI.25} + 
    \Sexpr{b33} X_{BP.100}\\[8pt]
    \text{logit}(\hat{\pi}_{mid.gluc}) &= 
    \Sexpr{b20} + \Sexpr{b21} X_{age.40} + \Sexpr{b22} X_{BMI.25} + 
    \Sexpr{b23} X_{BP.100}
  \end{align*}
\end{itemize}

\pagebreak

\begin{itemize}
\item Exponentiating the coefficients produces:
  \begin{align*}
    \frac{\hat{\pi}_{hi.gluc}}{\hat{\pi}_{low.gluc}} &= 
    \Sexpr{eB30} \times \Sexpr{eB31}^{X_{age.40}} \times \Sexpr{eB32}^{X_{BMI.25}} 
    \times \Sexpr{eB33}^{X_{BP.100}}\\[8pt]
    \frac{\hat{\pi}_{mid.gluc}}{\hat{\pi}_{low.gluc}} &= 
    \Sexpr{eB20} \times \Sexpr{eB21}^{X_{age.40}} \times \Sexpr{eB22}^{X_{BMI.25}} 
    \times \Sexpr{eB23}^{X_{BP.100}}
  \end{align*}
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%
  
\begin{frame}{}
  
  \begin{center}
    \Huge{\textsc{Classification}}
  \end{center}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Predictions from Logistic Regression}
  
  Given a fitted logistic regression model, we can get predictions for new 
  observations of $\{X_p\}$, $\{X_p'\}$.
  \vc
  \begin{itemize}
  \item Directly applying $\{\hat{\beta}_0, \hat{\beta}_p\}$ to $\{X_p'\}$ will 
    produce predictions on the scale of $\eta$:
    \begin{align*}
      \hat{\eta}' = \hat{\beta}_0 + \sum_{p = 1}^P \hat{\beta}_p X_p'
    \end{align*}
  \item By applying the inverse link function, $g^{-1}(\cdot)$, to
    $\hat{\eta}'$, we get predicted success probabilities:
    \begin{align*}
      \hat{\pi}' = g^{-1}(\hat{\eta}')
    \end{align*}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Predictions from Logistic Regression}
  
  In logistic regression, the inverse link function, $g^{-1}(\cdot)$, is the
  \emph{logistic function}:
  \begin{align*}
    \text{logistic}(X) = \frac{e^X}{1 + e^X}
  \end{align*}
  So, we convert $\hat{\eta}'$ to $\hat{\pi}'$ by:
  \begin{align*}
    \hat{\pi}' &= \frac{e^{\hat{\eta}'}}{1 + e^{\hat{\eta}'}} = \frac{\exp \left( 
      \hat{\beta}_0 + \sum_{p = 1}^P \hat{\beta}_p X_p' \right) }{1 + \exp \left( 
      \hat{\beta}_0 + \sum_{p = 1}^P \hat{\beta}_p X_p' \right) }
  \end{align*}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Classification with Logistic Regression}
  
  Once we have computed the predicted success probabilities, $\hat{\pi}'$, we 
  can use them to classify new observations.
  \vc
  \begin{itemize}
  \item By choosing a threshold on $\hat{\pi}'$, say $\hat{\pi}' = t$, we can 
    classify the new observations as ``Successes'' or ``Failures'':
    \begin{align*}
      \hat{Y}' = \left\{ 
      \begin{array}{ccc}
        1 & if & \hat{\pi}' \geq t\\
        0 & if & \hat{\pi}' < t\\
      \end{array}
      \right.
    \end{align*}
    
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Classification Example}
  
<<echo = FALSE>>=
xn   <- c(1, 57 - 40, 28 - 25, 92 - 100)
eta1 <- crossprod(xn, beta1)
pi1  <- round(exp(eta1) / (1 + exp(eta1)), 3)
eta1 <- round(eta1, 3)
g    <- as.numeric(pi1 > 0.5) + 1
@ 

Say we want to classify a new patient into either the ``high glucose'' group 
or the ``not high glucose'' group using the model fit above.
\begin{itemize}
\item Assume this patient has the following characteristics:
  \begin{itemize}
  \item They are 57 years old
  \item Their BMI is 28
  \item Their average blood pressure is 92
  \end{itemize}
\end{itemize}
\vb
First we plug their predictor data into the fitted model to get their 
model-implied $\eta$:
\begin{align*}
  \hat{\eta} &= \Sexpr{b0} + \Sexpr{b1} (57 - 40) + \Sexpr{b2} (28 - 25) + \Sexpr{b3} (92 - 100)\\
  &= \Sexpr{eta1} 
\end{align*}

\pagebreak

Next we convert the predicted $\eta$ value into a model-implied success
probability by applying the logistic function:
\begin{align*}
  \Sexpr{pi1} = \frac{e^{\Sexpr{eta1}}}{1 + e^{\Sexpr{eta1}}}
\end{align*}\\
\vb
Finally, to make the classification, assume a threshold of $\hat{\pi}' = 0.5$ as 
the decision boundary.
\begin{itemize}
\item Because $\Sexpr{pi1} \Sexpr{c("<", ">")[g]} 0.5$ we would classify this 
  patient into the ``\Sexpr{c("low", "high")[g]} glucose'' group.
\end{itemize}

\end{frame}
   
%------------------------------------------------------------------------------%

\begin{frame}{Predictions from Multinomial Logistic Regression}
  
  Generating predictions from a multinomial logistic regression model is nearly 
  identical to predicting with a logistic regression model.
  \vc
  \begin{itemize}
  \item The only difference is that the multinomial logistic regression model 
    will produce $L$ distinct estimates of $\hat{\eta}_l'$ and $\hat{\pi}_l'$:
    \begin{align*}
      \hat{\eta}_l' &= 
      \begin{cases}
        \hat{\beta}_{l0} + \sum_{p = 1}^P \hat{\beta}_{lp} X_p' & \text{if $l > 1$}\\
        0 & \text{if $l = 1$}
      \end{cases}\\[8pt]
      \hat{\pi}_l' &= g^{-1}(\hat{\eta}_l')
    \end{align*}
  \end{itemize}
  
  %\item The success probability for the baseline level, $\hat{\pi}_1$, is defined 
  %  by the other $L - 1$ $\hat{\pi}_l$ values:
  %  \begin{align*}
  %    \hat{\pi}_1 = 1 - \sum_{l = 2}^L\hat{\pi}_l
  %  \end{align*}
  %\end{itemize}
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Predictions from Multinomial Logistic Regression}
  
  In multinomial logistic regression, the inverse link function, $g^{-1}(\cdot)$, 
  is the \emph{softmax function}:
  \begin{align*}
    \text{softmax}(X_l) = \frac{e^{X_l}}{\sum_{j = 1}^Je^{X_j}}
  \end{align*}
  So, we convert each $\hat{\eta}_l'$ to $\hat{\pi}_l'$ by:
  \begin{align*}
    \hat{\pi}_l' = \frac{e^{\hat{\eta}_l'}}{\sum_{j = 1}^J e^{\hat{\eta}_j'}} = 
    \begin{cases}
      \frac{\exp \left( 
        \hat{\beta}_{l0} + \sum_{p = 1}^P \hat{\beta}_{lp} X_p' \right) }
           {1 + \sum_{j = 2}^J \exp \left(
             \hat{\beta}_{j0} + \sum_{p = 1}^P \hat{\beta}_{jp} X_p' 
             \right) } & \text{if $l > 1$}\\[16pt]
           \frac{1}
                {1 + \sum_{j = 2}^J \exp \left(
                  \hat{\beta}_{j0} + \sum_{p = 1}^P \hat{\beta}_{jp} X_p' 
                  \right) } & \text{if $l = 1$}
    \end{cases}
  \end{align*}
           
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Classification with Multinomial Logistic Regression}
  
  Once we have computed the $L$ predicted success probabilities, $\hat{\pi}_l'$, 
  we can use them to classify new observations.
  \vc
  \begin{itemize}
  \item Each observation is labeled with the response level associated with the 
    largest $\hat{\pi}_l'$
    \vc
  \item For example:
    \begin{itemize}
    \item Given the response options $Y \in $
      \{Coffee, Tea, Energy Drinks, None\}
      \vc
    \item And corresponding success probabilities $\hat{\pi}_l \in 
      \{0.45, 0.2, 0.15, 0.2\}$
      \vc
    \item We would assign the observation to the ``Coffee'' group 
    \end{itemize}
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Classification Example}
  
<<echo = FALSE>>=
beta2           <- rbind(0, beta2)
rownames(beta2) <- c("low", "mid", "hi")

eta2 <- apply(beta2, 1, function(x, y) crossprod(y, x), y = xn)
pi2  <- exp(eta2) / sum(exp(eta2))

eta2 <- round(eta2, 3)
pi2  <- round(pi2, 3)

b10 <- round(beta2[1, 1], 3)
b11 <- round(beta2[1, 2], 3)
b12 <- round(beta2[1, 3], 3)
b13 <- round(beta2[1, 4], 3)

classNum  <- which.max(pi2)
className <- names(classNum)
@ 

Let's re-classify our patient into either the ``high glucose'', ``moderate 
glucose'', or ``low glucose'' group using the model fit above.
\begin{itemize}
\item First we plug their predictor data into the fitted model to get their 
  set of model-implied $\eta_l$ values:
\end{itemize}
%\begin{align*}
%  \text{logit}\left(\frac{\pi_{low.gluc}}{\pi_{low.gluc}}\right) &= 
%  \Sexpr{b10} &+ \Sexpr{b11} (57 - 40) &+ \Sexpr{b12} (28 - 25)\\ &+ \Sexpr{b13} (92 - 100) &= 
%  \Sexpr{eta2[1]}\\
%  \text{logit}\left(\frac{\pi_{mid.gluc}}{\pi_{low.gluc}}\right) &=
%  \Sexpr{b20} &+ \Sexpr{b21} (57 - 40) &+ \Sexpr{b22} (28 - 25)\\ &+ \Sexpr{b23} (92 - 100) &=
%  \Sexpr{eta2[2]}\\
%  \text{logit}\left(\frac{\pi_{hi.gluc}}{\pi_{low.gluc}}\right) &=
%  \Sexpr{b30} &+ \Sexpr{b31} (57 - 40) &+ \Sexpr{b32} (28 - 25)\\ &+ \Sexpr{b33} (92 - 100) &=
%  \Sexpr{eta2[3]}
%\end{align*}
%\end{itemize}

\vc

\setlength\tabcolsep{1.5pt} % decrease column padding in tabular

\begin{tabular}{rllll}
  $\text{logit}\left(\frac{\pi_{low.gluc}}{\pi_{low.gluc}}\right)$ &=&&&\\ 
  \Sexpr{b10} &+ \Sexpr{b11} (57 - 40) &+ \Sexpr{b12} (28 - 25) &+ \Sexpr{b13} (92 - 100) &= 
  \Sexpr{eta2[1]}\\[8pt]
  $\text{logit}\left(\frac{\pi_{mid.gluc}}{\pi_{low.gluc}}\right)$ &=&&&\\
  \Sexpr{b20} &+ \Sexpr{b21} (57 - 40) &+ \Sexpr{b22} (28 - 25) &+ \Sexpr{b23} (92 - 100) &=
  \Sexpr{eta2[2]}\\[8pt]
  $\text{logit}\left(\frac{\pi_{hi.gluc}}{\pi_{low.gluc}}\right)$ &=&&&\\
  \Sexpr{b30} &+ \Sexpr{b31} (57 - 40) &+ \Sexpr{b32} (28 - 25) &+ \Sexpr{b33} (92 - 100) &=
  \Sexpr{eta2[3]}
\end{tabular}

\setlength\tabcolsep{6pt} % back to the default value

\pagebreak

\begin{itemize}
\item Next we apply the softmax function to convert the predicted $\eta_l$ 
  values into model-implied success probabilities:
\end{itemize}
\begin{align*}
  \hat{\pi}_{low.gluc} &=  
  \frac{1}{1 + e^{\Sexpr{eta2[2]}} + e^{\Sexpr{eta2[3]}}} = 
  \Sexpr{pi2[1]}\\[8pt]
  \hat{\pi}_{mid.gluc} &= 
  \frac{e^{\Sexpr{eta2[2]}}}{1 + e^{\Sexpr{eta2[2]}} + e^{\Sexpr{eta2[3]}}} = 
  \Sexpr{pi2[2]}\\[8pt]
  \hat{\pi}_{hi.gluc} &= 
  \frac{e^{\Sexpr{eta2[3]}}}{1 + e^{\Sexpr{eta2[2]}} + e^{\Sexpr{eta2[3]}}} =
  \Sexpr{pi2[3]}  
\end{align*}
\begin{itemize}
\item Finally, to make the classification, we find the largest $\hat{\pi}_l'$:
  \begin{itemize}
  \item Because $\hat{\pi}_{\Sexpr{className}.gluc} = \Sexpr{pi2[classNum]}$ is the 
    largest, we would classify this patient into the 
    ``\Sexpr{c('low', 'moderate', 'high')[classNum]} glucose'' group.
  \end{itemize}
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Classification Error}
  
  The MSE is not an appropriate error measure for classification.
  \begin{itemize}
  \item The differences between predicted and observed outcomes have little
    meaning.
  \end{itemize}
  \vb
  One of the most popular error measures is the \emph{Cross-Entropy Error}:
  \begin{align*}
    CEE &= -N^{-1} \sum_{n = 1}^N Y_n \ln(\hat{\pi}_n) + (1 - Y_n)\ln(1 - \hat{\pi}_n)\\
    CEE &= -N^{-1} \sum_{n = 1}^N \sum_{l = 1}^L \textbf{I}(Y_n = l) \ln(\hat{\pi}_{nl})
  \end{align*}
  \vx{-6}
  \begin{itemize}
  \item The CEE is sensitive to classification confidence.
  \item Stronger predictions are more heavily weighted.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Why not Missclassification Rate?}
  
<<echo = FALSE>>=
yTrue <- rbinom(100, 1, 0.5)

pi1   <- yTrue * 0.9 + (1 - yTrue) * 0.1
pred1 <- as.numeric(pi1 > 0.5)

pi2   <- yTrue * 0.55 + (1 - yTrue) * 0.45
pred2 <- as.numeric(pi2 > 0.5)

cee1 <- LogLoss(y_pred = pi1, y_true = yTrue)
cee2 <- LogLoss(y_pred = pi2, y_true = yTrue)
@ 

 The missclassification rate is a na\"{i}vely appealing option.
  \begin{itemize}
  \item The proportion of cases assigned to the wrong group
  \end{itemize}
  \vb
  Consider two perfect classifiers:
  \begin{enumerate}
  \item $P(\hat{Y}_n = 1 | Y_n = 1) = 0.90$,  $P(\hat{Y}_n = 1 | Y_n = 0) = 0.10$, $n = 1, 2, \ldots, N$
  \item $P(\hat{Y}_n = 1 | Y_n = 1) = 0.55$,  $P(\hat{Y}_n = 1 | Y_n = 0) = 0.45$, $n = 1, 2, \ldots, N$
  \end{enumerate}
  \vb
  Both of these classifiers will have the same missclassification rate.
  \begin{itemize}
  \item Neither model ever makes an incorrect group assignment.
  \end{itemize}
  \vb
  The first model will have a lower CEE.
  \begin{itemize}
  \item The classifications are made with higher confidence.
  \item $CEE_1 = \Sexpr{round(cee1, 3)}$, $CEE_2 = \Sexpr{round(cee2, 3)}$
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Conclusion}
  
  \begin{itemize}
  \item The Generalized Linear Model is a flexible class of models that we can
    use for non-normally distributed outcomes.
    \begin{itemize}
    \item Multiple linear regression is a special type of GLM.
    \end{itemize}
    \vc
  \item We cannot model nominal outcomes with linear regression.
    \begin{itemize}
    \item We should use some form of logistic regression.
    \end{itemize}
    \vc
  \item We use logistic regression for binary outcomes and multinomial logistic
    regression for multi-class nominal outcomes.
    \vc
  \item We must take care when interpreting the coefficients from logistic
    regression models.
    \vc
  \item We can use the estimated success probabilities from a fitted model to
    classify new observations.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\end{document}

