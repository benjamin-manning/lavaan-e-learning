\documentclass{beamer}
\usetheme{TTU}
\usefonttheme{serif}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage[natbibapa]{apacite}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{Sweavel}
\usepackage{listings}
\usepackage{fancybox}

\def\Sweavesize{\scriptsize}
\def\Rcolor{\color{black}}
%\def\Routcolor{\color{red}}
\def\Rcommentcolor{\color{violet}}
\def\Rbackground{\color[gray]{0.85}}
\def\Routbackground{\color[gray]{0.85}}

\lstset{tabsize=2, breaklines=true, style=Rstyle}

\SweaveOpts{keep.source=T, prefix.string=sweaveFiles/lecture12, split=T, ae=F, height=4, width=6}

\newcommand{\red}[0]{\textcolor{red}}
\newcommand{\green}[0]{\textcolor{green}}
\newcommand{\blue}[0]{\textcolor{blue}}
\newcommand{\comment}[1]{}
\newcommand{\va}[0]{\vspace{12pt}}
\newcommand{\vb}[0]{\vspace{6pt}}
\newcommand{\vc}[0]{\vspace{3pt}}
\newcommand{\vx}[1]{\vspace{#1pt}}

\title[Lecture 12]{Lecture 12: Tying Up Loose Ends}

\author{Kyle M. Lang}

\institute[TTU IMMAP]{
  Institute for Measurement, Methodology, Analysis \& Policy\\
  Texas Tech University\\
  Lubbock, TX
}

\date{May 2, 2016}

\setbeamertemplate{frametitle continuation}{}

\begin{document}

\setkeys{Gin}{width=\textwidth}

<<echo=F>>=
options(width=160, prompt=" ", continue=" ", useFancyQuotes = TRUE)
library(xtable)
@


% Use a custom background template for the title slide
{\usebackgroundtemplate{\rule{0pt}{3.4in}\hspace*{2.25in}
    \makebox[0pt][r]{\includegraphics[natwidth=1000bp, natwidth=283bp,width=2in]{TTU_Logo.png}}}

  \begin{frame}[plain]

    \titlepage

  \end{frame}
}% CLOSE Custom Background Template


\begin{frame}{Outline}

  \begin{itemize}
  \item Latent variable interactions
    \va
  \item Moderated logistic regression
    \va
  \item Effect size for conditional process analysis
  \end{itemize}
  
\end{frame}


\begin{frame}[shrink = 5]{Latent Variable Interactions}
  
  When we have two observed variables interacting to predict a latent
  variable, our job is easy:
  \begin{enumerate}
    \item Construct the product term of the observed focal and
      moderator variables
    \item Use the observed focal, moderator, and interaction variables
      to predict the latent DV
  \end{enumerate}\\
  \va If we want to model moderation when at least on of the
  predictors is latent, things get more difficult.
  \begin{itemize}
  \item If the moderator is observed and discrete, we can use multiple
    group modeling
  \item If the moderator is continuous and/or latent, then we need
    fancier methods
  \end{itemize}\\
  \va
  Two basic approaches:
  \begin{enumerate}
  \item Methods based on products of manifest variables
  \item Methods based on directly estimating the products of latent
    variables
  \end{enumerate}

\end{frame}


\begin{frame}{Estimating Products of Latent Variables}
  
  We can directly estimate the interaction between two latent
  variables with the \emph{latent moderated structural equations}
  (LMS) method.
  \va
  \begin{itemize}
  \item Introduced by \citet{kleinEtAl:1997} and formalized by
    \citet{kleinMoosbrugger:2000} 
    \vb
  \item Currently only available in Mplus (via the \texttt{Xwith}
    command).
    \vb
  \item Uses numerical integration to estimate the unobserved latent
    interaction term
  \end{itemize}
  
\end{frame}


\begin{frame}{Estimating Products of Latent Variables}
  
  \textsc{LMS Strengths:}
  \begin{itemize}
    \item Tends to perform the best out of all available methods
      \vb
    \item No need to pre-process the data by manually computing
      product terms
      \vb
    \item Pretty easy to implement if you have Mplus (see users guide
      for examples).
  \end{itemize}
  \va
  \textsc{LMS Weaknesses:}
  \begin{itemize}
  \item Only available in one (proprietary) software package
    \vb
  \item Numerical integration is very slow and precludes calculation
    of most fit indices
    \vb
  \item LMS does not work with categorical observed moderators
  \end{itemize}
  
\end{frame}


\begin{frame}{Computing Interaction Indicators}
  
  The alternative to the LMS-type approach is to create observed
  product terms and directly use those terms as indicators of the
  interaction construct.
  \vb
  \begin{itemize}
  \item Naively indicating an interaction construct with the raw
    product terms is probably sub-optimal
    \vb
  \item Collinearity among the interaction indicators and the raw
    items can cause estimation problems
    \vb
  \item From a modeling perspective, we'd like to interpret out final
    model holistically
  \end{itemize}
  \va
  Two recommended approaches:
  \vb
  \begin{enumerate}
  \item Orthogonalization through residual centering
    \citep{littleEtAl:2006}.
    \vb
  \item Double mean centering \citep{linEtAl:2010}.
  \end{enumerate}
  
\end{frame}


\begin{frame}[allowframebreaks]{Orthogonalization}
  
  Say we want to estimate the moderated effect of $Z$ on the $X
  \rightarrow Y$ effect, where $X$, $Y$, and $Z$ are latent variables
  indicated by $\{x_1, x_2, x_3\}$, $\{y_1, y_2, y_3\}$, and $\{z_1,
  z_2, z_3\}$, respectively.\\
  \va
  Orthogonalization is performed by:
  \vb
  \begin{enumerate}
  \item Construct all possible product terms: $\{x_1z_1, x_1z_2, x_1z_3, x_2z_1, x_2z_2, x_2z_3, x_3z_1, x_3z_2, x_3z_3\}$.
    \vb
  \item Regress each product term onto all observed indicators of $X$ and $Z$:
    \begin{align*}
      \widehat{x_1z_1} &= \alpha + \beta_1x_1 + \beta_2x_2 + \beta_2x_3 + 
      \beta_4z_1 + \beta_5z_2 + \beta_6z_3\\
      \widehat{x_2z_1} &= \alpha + \beta_1x_1 + \beta_2x_2 + \beta_2x_3 + 
      \beta_4z_1 + \beta_5z_2 + \beta_6z_3\\
      &~~~\vdots\\
      \widehat{x_3z_3} &= \alpha + \beta_1x_1 + \beta_2x_2 + \beta_2x_3 + 
      \beta_4z_1 + \beta_5z_2 + \beta_6z_3
    \end{align*}
    
    \pagebreak
    
  \item Calculate each product term's residual:
    \begin{align*}
      \delta_{x1z1} &= x_1z_1 - \widehat{x_1z_1}\\
      \delta_{x1z1} &= x_2z_1 - \widehat{x_2z_1}\\
      &~~~\vdots\\
      \delta_{x3z3} &= x_3z_3 - \widehat{x_3z_3}
    \end{align*}
    \vb
  \item Use these residuals to indicate a latent interaction construct
    as represented in the following figure.
  \end{enumerate}
  
\end{frame}


\begin{frame}{Orthogonalization}
  
  \begin{figure}
    \includegraphics[width=\textwidth]{figures/orthoDiagram.pdf} 
  \end{figure}
  
\end{frame}



\begin{frame}[allowframebreaks]{Example}
  
<<>>=
library(lavaan)
dat1 <- readRDS("../data/lecture12Data.rds")

mod1 <- "
fX =~ x1 + x2 + x3
fZ =~ z1 + z2 + z3
fY =~ y1 + y2 + y3
"

out1 <- cfa(mod1, data = dat1, std.lv = TRUE)
summary(out1)
@ 

\pagebreak

<<>>=
round(fitMeasures(out1)[c("chisq", "df", "pvalue", "cfi", 
                          "tli", "rmsea", "srmr")], 3)
@

\end{frame}


\begin{frame}[allowframebreaks]{Example}
  
<<>>=
mod2 <- "
fX =~ x1 + x2 + x3
fZ =~ z1 + z2 + z3
fY =~ y1 + y2 + y3

fY ~ fX + fZ
"

out2 <- sem(mod2, data = dat1, std.lv = TRUE)
summary(out2)
@ 

\pagebreak

<<>>=
round(fitMeasures(out2)[c("chisq", "df", "pvalue", "cfi", 
                          "tli", "rmsea", "srmr")], 3)
@

\end{frame}


\begin{frame}[allowframebreaks]{Example}
  
<<>>=
predDat <- as.matrix(dat1[ , -grep("y", colnames(dat1))])
dat2 <- dat1

## Construct product terms:
x1z1 <- with(dat2, x1*z1)
x1z2 <- with(dat2, x1*z2)
x1z3 <- with(dat2, x1*z3)

x2z1 <- with(dat2, x2*z1)
x2z2 <- with(dat2, x2*z2)
x2z3 <- with(dat2, x2*z3)

x3z1 <- with(dat2, x3*z1)
x3z2 <- with(dat2, x3*z2)
x3z3 <- with(dat2, x3*z3)

## Residualize the product terms:
dat2$x1z1R <- lm(x1z1 ~ predDat)$resid
dat2$x1z2R <- lm(x1z2 ~ predDat)$resid
dat2$x1z3R <- lm(x1z3 ~ predDat)$resid

dat2$x2z1R <- lm(x2z1 ~ predDat)$resid
dat2$x2z2R <- lm(x2z2 ~ predDat)$resid
dat2$x2z3R <- lm(x2z3 ~ predDat)$resid

dat2$x3z1R <- lm(x3z1 ~ predDat)$resid
dat2$x3z2R <- lm(x3z2 ~ predDat)$resid
dat2$x3z3R <- lm(x3z3 ~ predDat)$resid
@ 

\pagebreak

<<>>=
mod3 <- "
fX =~ x1 + x2 + x3
fZ =~ z1 + z2 + z3
fY =~ y1 + y2 + y3
fXZ =~ x1z1R + x1z2R + x1z3R +
x2z1R + x2z2R + x2z3R +
x3z1R + x3z2R + x3z3R

fY ~ fX + fZ + fXZ

fX ~~ fZ
fX ~~ 0*fXZ
fZ ~~ 0*fXZ

x1z1R ~~ x1z2R + x1z3R + x2z1R + x3z1R
x1z2R ~~ x1z3R + x2z2R + x3z2R
x1z3R ~~ x2z3R + x3z3R

x2z1R ~~ x2z2R + x2z3R + x3z1R
x2z2R ~~ x2z3R + x3z2R
x2z3R ~~ x3z3R

x3z1R ~~ x3z2R + x3z3R
x3z2R ~~ x3z3R
"

out3 <- sem(mod3, data = dat2, std.lv = TRUE)
summary(out3)
@ 

\pagebreak

<<>>=
round(fitMeasures(out3)[c("chisq", "df", "pvalue", "cfi", 
                          "tli", "rmsea", "srmr")], 3)
library(semTools)

out3.2 <- 
    sem(mod3, data = dat2, std.lv = TRUE, meanstructure = TRUE)
probeOut3 <- probe2WayRC(fit = out3.2,
                         nameX = c("fX", "fZ", "fXZ"),
                         nameY = "fY",
                         modVar = "fZ",
                         valProbe = c(-1, 0, 1)
                         )
probeOut3$SimpleSlope
@ 

\end{frame}



\begin{frame}{Matched Pair Variation}
  
  If you are willing to assume exchangeable indicators (i.e.,
  \emph{essential tau equivalence}), then you don't need to compute
  all possible interaction terms.\\ 
  \va 
  The so-called \emph{matched pair} strategy suggests constructing only 
  three product variables (when each first order construct has three indicators).
  \va
  \begin{itemize}
    \item Each product variable is simply constructed from paired
      indicators of the two first-order constructs:
      \begin{align*}
        x_1z_1 &= x_1 \times z_1\\
        x_2z_2 &= x_2 \times z_2\\
        x_3z_3 &= x_3 \times z_3
      \end{align*}
  \end{itemize}   

\end{frame}
  


\begin{frame}[allowframebreaks]{Example}
  
<<>>=
mod4 <- "
fX =~ x1 + x2 + x3
fZ =~ z1 + z2 + z3
fY =~ y1 + y2 + y3
fXZ =~ x1z1R + x2z2R + x3z3R

fY ~ fX + fZ + fXZ

fX ~~ fZ
fX ~~ 0*fXZ
fZ ~~ 0*fXZ
"

out4 <- 
    sem(mod4, data = dat2, std.lv = TRUE, meanstructure = TRUE)
summary(out4)
@ 

\pagebreak

<<>>=
round(fitMeasures(out4)[c("chisq", "df", "pvalue", "cfi", 
                          "tli", "rmsea", "srmr")], 3)

fitMeasures(out3)[c("aic", "bic")]
fitMeasures(out4)[c("aic", "bic")]

probeOut4 <- probe2WayRC(fit = out4,
                         nameX = c("fX", "fZ", "fXZ"),
                         nameY = "fY",
                         modVar = "fZ",
                         valProbe = c(-1, 0, 1)
                         )

probeOut4$SimpleSlope
@

\end{frame}



\begin{frame}[allowframebreaks]{Double Mean Centering}
  
  Using the same problem setup as above, we could perform double mean
  centering by:\\ 
  \vb
  \begin{enumerate}
  \item Mean center every indicator of $X$ and $Z$:
    \begin{align*}
      x_1^c &= x_1 - \bar{x}_1\\
      &~~~\vdots\\
      z_1^c &= z_1 - \bar{z}_1\\
      &~~~\vdots
    \end{align*}
  \item Use the centered indicators to construct all possible product
    terms: $\{x_1^cz_1^c,$ $x_1^cz_2^c,$ $x_1^cz_3^c,$ $x_2^cz_1^c,$
    $x_2^cz_2^c,$ $x_2^cz_3^c,$ $x_3^cz_1^c,$ $x_3^cz_2^c,$ $x_3^cz_3^c\}$.
    
    \pagebreak
    
  \item Mean center each product term:
    \begin{align*}
      (x_1z_1)^c &= x_1^cz_1^c - \overline{x_1^cz_1^c}\\
      (x_1z_2)^c &= x_1^cz_2^c - \overline{x_1^cz_2^c}\\
      &~~~\vdots\\
      (x_3z_3)^c &= x_3^cz_3^c - \overline{x_3^cz_3^c}
    \end{align*}
    \vb
  \item Use the mean centered indicators of $X$ and $Z$, and the
    ``double mean centered'' product terms to specify the latent
    interaction model as represented in the following figure.
  \end{enumerate}
  
\end{frame}


\begin{frame}{Double Mean Centering}
  
  \begin{figure}
    \includegraphics[width=\textwidth]{figures/dmcDiagram.pdf}
  \end{figure}
  
\end{frame}


\begin{frame}[allowframebreaks]{Example}
  
<<>>=
dat3 <- data.frame(lapply(dat1, scale, scale = FALSE)) 

tmpDat <- data.frame(
    x1z1 = with(dat3, x1*z1),
    x1z2 = with(dat3, x1*z2),
    x1z3 = with(dat3, x1*z3),
    
    x2z1 = with(dat3, x2*z1),
    x2z2 = with(dat3, x2*z2),
    x2z3 = with(dat3, x2*z3),
    
    x3z1 = with(dat3, x3*z1),
    x3z2 = with(dat3, x3*z2),
    x3z3 = with(dat3, x3*z3)
)

dat3 <- data.frame(dat3,
                   lapply(tmpDat, scale, scale = FALSE)
                   )
@ 

\pagebreak

<<>>=
mod5 <- "
fX =~ x1 + x2 + x3
fZ =~ z1 + z2 + z3
fY =~ y1 + y2 + y3
fXZ =~ x1z1 + x1z2 + x1z3 +
       x2z1 + x2z2 + x2z3 +
       x3z1 + x3z2 + x3z3

fY ~ fX + fZ + fXZ

fX ~~ fZ

x1z1 ~~ x1z2 + x1z3 + x2z1 + x3z1
x1z2 ~~ x1z3 + x2z2 + x3z2
x1z3 ~~ x2z3 + x3z3

x2z1 ~~ x2z2 + x2z3 + x3z1
x2z2 ~~ x2z3 + x3z2
x2z3 ~~ x3z3

x3z1 ~~ x3z2 + x3z3
x3z2 ~~ x3z3
"

out5 <- sem(mod5, data = dat3, std.lv = TRUE)
summary(out5)
@ 

\pagebreak

<<>>=
round(fitMeasures(out5)[c("chisq", "df", "pvalue", "cfi", 
                          "tli", "rmsea", "srmr")], 3)

out5.2 <- 
    sem(mod5, data = dat3, std.lv = TRUE, meanstructure = TRUE)

probeOut5 <- probe2WayMC(fit = out5.2,
                         nameX = c("fX", "fZ", "fXZ"),
                         nameY = "fY",
                         modVar = "fZ",
                         valProbe = c(-1, 0, 1)
                         )
probeOut5$SimpleSlope
@

\end{frame}


\begin{frame}[allowframebreaks]{Example}
  
<<>>=
mod6 <- "
fX =~ x1 + x2 + x3
fZ =~ z1 + z2 + z3
fY =~ y1 + y2 + y3
fXZ =~ x1z1 + x2z2 + x3z3

fY ~ fX + fZ + fXZ

fX ~~ fZ
"

out6 <- 
    sem(mod6, data = dat3, std.lv = TRUE, meanstructure = TRUE)
summary(out6)
@ 

\pagebreak

<<>>=
round(fitMeasures(out6)[c("chisq", "df", "pvalue", "cfi", 
                          "tli", "rmsea", "srmr")], 3)

fitMeasures(out5)[c("aic", "bic")]
fitMeasures(out6)[c("aic", "bic")]

probeOut6 <- probe2WayMC(fit = out6,
                         nameX = c("fX", "fZ", "fXZ"),
                         nameY = "fY",
                         modVar = "fZ",
                         valProbe = c(-1, 0, 1)
                         )

probeOut6$SimpleSlope
@

\end{frame}


\begin{frame}{Orthogonalization vs. Double Mean Centering}
  
  Orthogonalization and double mean centering tend to behave
  comparably, but each has its own strengths: 
  \vb
  \begin{itemize}
    \item When $X$ and $Z$ are bivariate normally distributed, both
      methods produce the same results.
      \vb
    \item As $X$ and/or $Z$ stray from normality, orthogonalization
      produces biased estimates of the interaction effect, but double
      mean centering does not.
      \vb
    \item Orthogonalization ensures that the latent $XZ$ is perfectly
      independent of $X$ and $Z$.
      \vc
      \begin{itemize}
        \item The $X$ and $Z$ parameters can be directly interpreted,
          without any conditioning
      \end{itemize}
  \end{itemize}
  
\end{frame}



\begin{frame}[allowframebreaks]{Example}
  
<<>>=
## Use semTools to orthogonalize:
dat2.2 <- indProd(data = dat1,
                  var1 = c("x1", "x2", "x3"),
                  var2 = c("z1", "z2", "z3"),
                  match = FALSE,
                  residualC = TRUE)

sum(dat2 - dat2.2)
##
## Use semTools to double mean center:
dat3.2 <- indProd(data = dat1,
                  var1 = c("x1", "x2", "x3"),
                  var2 = c("z1", "z2", "z3"),
                  match = FALSE,
                  doubleMC = TRUE)

sum(dat3[ , -c(1 : 9)] - dat3.2[ , -c(1 : 9)])
@ 

\end{frame}


\begin{frame}{Other Things}
  
  Moderation in logistic regression:
  \vb
  \begin{itemize}
    \item Nothing special
      \vc
    \item Just include the product term as a predictor
      \vc
    \item Make sure to keep track of the weird ``multiplicative change
      in log-odds'' interpretation of your coefficients
  \end{itemize}
  \va
  \pause
  Effect size for conditional process analysis:
  \vb
  \begin{itemize}
    \item We don't know
      \vc
    \item I could not find any work directly addressing the issue
      \vc
    \item Fully and partially standardized indirect effects seem like
      they should still work
      \vc
    \item $\kappa^2$ and the various flavors of $R^2$ aren't so
      clear-cut.
  \end{itemize}
  
\end{frame}



\begin{frame}{References}

  \bibliographystyle{apacite}
  \bibliography{bibtexStuff/lecture12Refs.bib}

\end{frame}


\end{document}
