%%% Title:    Lavaan E-Learning: Missing Data
%%% Author:   Kyle M. Lang
%%% Created:  2015-11-06
%%% Modified: 2022-06-14

\documentclass{beamer}
\usetheme{Utrecht}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{fancybox}
\usepackage{booktabs}
\usepackage{eurosym}
\usepackage{caption}

\captionsetup{labelformat = empty}

\newcommand{\eqit}[1]{\textrm{\textit{#1}}}
\newcommand{\pkg}[1]{\textbf{#1}}
\newcommand{\src}[1]{\texttt{#1}}
\newcommand{\rmsc}[1]{\textrm{\textsc{#1}}}
\newcommand{\mub}[0]{\boldsymbol{\muup}}

\title{Missing Data in SEM}
\subtitle{Introduction to SEM with Lavaan}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{}

%------------------------------------------------------------------------------%

\begin{document}

<<setup, include = FALSE, cache = FALSE>>=
set.seed(235711)

library(knitr)
library(ggplot2)
library(mice)
library(mvtnorm)
library(xtable)
library(pROC)
library(dplyr)
library(magrittr)
library(naniar)
library(ggpubr)
library(lme4)
library(lavaan)
library(LaplacesDemon)
library(semTools)
library(optimx)
library(mgcv)

source("../../code/supportFunctions.R")
source("../../code/sim_missing/code/simMissingness.R")

dataDir <- "../data/"

options(width = 80)
opts_chunk$set(size = "footnotesize",
               fig.align = "center",
               fig.path = "figure/missing_data-",
               message = FALSE,
               warning = FALSE,
               comment = "")
knit_theme$set('edit-kwrite')
@

%------------------------------------------------------------------------------%

\begin{frame}[t, plain]
  \titlepage
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{What are Missing Data?}

  Missing data are empty cells in a dataset where there should be observed
  values.
  \vc
  \begin{itemize}
  \item The missing cells correspond to true population values, but we haven't
    observed those values.
  \end{itemize}
  \vb
  \pause
  Not every empty cell is a missing datum.
  \vc
  \begin{itemize}
  \item Quality-of-life ratings for dead patients in a mortality study
    \vc
  \item Firm profitability after the company goes out of business
    \vc
  \item Self-reported severity of menstrual cramping for men
    \vc
  \item Empty blocks of data following ``gateway'' items
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{A Little Notation}

  \begin{align*}
    Y &\coloneqq \text{An $N \times P$ Matrix of Arbitrary Data}\\[8pt]
    Y_{mis} &\coloneqq \text{The \emph{missing} part of $Y$}\\[8pt]
    Y_{obs} &\coloneqq \text{The \emph{observed} part of $Y$}\\[8pt]
    R &\coloneqq \text{An $N \times P$ response matrix}\\[8pt]
    M &\coloneqq \text{An $N \times P$ missingness matrix}
  \end{align*}

  The $R$ and $M$ matrices are complementary.
  \begin{itemize}
  \item $r_{np} = 1$ means $y_{np}$ is observed; $m_{np} = 1$ means $y_{np}$ is
    missing.
  \item $r_{np} = 0$ means $y_{np}$ is missing; $m_{np} = 0$ means $y_{np}$ is
    observed.
  \item $M_p$ is the \emph{missingness} of $Y_p$.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

<<>>=
## Load some useful packages:
library(dplyr)
library(naniar)
library(ggmice)

## Read in some data:
bfi0 <- readRDS("../data/bfi_datasets.rds")
bfi  <- bfi0$incomplete %>% select(-matches("N\\d|C\\d|E\\d|male"))

## Compute the variablewise proportions of missing data:
bfi %>% is.na() %>% colMeans() %>% round(2)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

Visualize the percentages missing via \pkg{naniar}\src{::gg\_miss\_var()}.
<<out.width = "50%">>=
gg_miss_var(bfi, show_pct = TRUE)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

Visualize the missing data patterns via \pkg{ggmice}\src{::plot\_pattern()}.
<<out.width = "50%">>=
plot_pattern(bfi)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

In \pkg{lavaan}, we can directly fit a model with incomplete data.

<<>>=
library(lavaan)

## Specify the measurement model:
cfaMod <- "
agree =~ A1 + A2 + A3 + A4 + A5
open  =~ O1 + O2 + O3 + O4 + O5
"

## Estimate the model:
out <- cfa(cfaMod, data = bfi, std.lv = TRUE)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

The model will estimate, just fine...

<<echo = FALSE>>= 
partSummary(out, c(1, 5))
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<echo = FALSE>>= 
partSummary(out, 6:7, 1:17)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

But not everything is as it seems.

<<echo = FALSE>>= 
partSummary(out, 2)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Default Approach}

Like most software packages, \pkg{lavaan} will default to \emph{complete case
    analysis} when asked to analyze incomplete data.

\vc

\begin{itemize}
\item In the absence of user input, this is a sensible option.
\vc
\item That doesn't mean you should actually use deletion to treat the missing
data in your analysis.
\end{itemize}

\vb
\pause

Complete case analysis has two major problems.
\vc
\begin{enumerate}
\item Throws out useful information (potentially a lot of information)
\vc
\item Probably biases parameter estimates.
\end{enumerate}

\vb

To understand the second point, we need to discuss \emph{missing data mechanisms}.

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\sectionslide{Missing Data Mechanisms}

%------------------------------------------------------------------------------%

\begin{frame}{Missing Data Mechanisms}

  Missing Completely at Random (MCAR)
  \begin{itemize}
  \item $P(R | Y_{mis}, Y_{obs}) = P(R)$
    \vc
  \item Missingness is unrelated to any study variables.
  \end{itemize}
  \vb
  %\begin{align*}
  %  P(R | Y_{mis}, Y_{obs}) = P(R)
  %\end{align*}

  Missing at Random (MAR)
  \begin{itemize}
  \item $P(R | Y_{mis}, Y_{obs}) = P(R | Y_{obs})$
    \vc
  \item Missingness is related to only the \emph{observed} parts of study
      variables.
  \end{itemize}
  \vb
  %\begin{align*}
  %  P(R | Y_{mis}, Y_{obs}) = P(R | Y_{obs})
  %\end{align*}

  Missing not at Random (MNAR)
  \begin{itemize}
  \item $P(R | Y_{mis}, Y_{obs}) \neq P(R | Y_{obs})$
    \vc
  \item Missingness is related to the \emph{unobserved} parts of study
    variables.
  \end{itemize}
  %\begin{align*}
  %  P(R | Y_{mis}, Y_{obs}) \neq P(R | Y_{obs})
  %\end{align*}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Simulate Some Toy Data}

<<>>=
nObs <- 5000 # Sample Size
pm   <- 0.3  # Proportion Missing

sigma <- matrix(c(1.0, 0.5, 0.3,
                  0.5, 1.0, 0.0,
                  0.3, 0.0, 1.0),
                ncol = 3)
tmp <- rmvnorm(nObs, c(0, 0, 0), sigma)

x0 <- tmp[ , 1]
y0 <- tmp[ , 2]
z0 <- tmp[ , 3]

cor(y0, x0) # Check correlation between X and Y
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{MCAR Example}

<<>>=
## Simulate MCAR Missingness:
mVec <- sample(1 : length(y0), size = pm * length(y0))

yMcar       <- y0
yMcar[mVec] <- NA

cor(yMcar, x0, use = "pairwise") # Look at correlation
@

\pagebreak

<<echo = FALSE, out.width = "65%">>=
y0Den <- density(y0)
yDen  <- density(yMcar, na.rm = TRUE)

pDat <- data.frame(y = c(y0Den$y, yDen$y),
                   x = c(y0Den$x, yDen$x),
                   g = rep(c("Complete", "MCAR w/ Deletion"),
                           each = length(yDen$y)
                           )
                   )

ggplot(data = pDat, mapping = aes(x = x, y = y, color = g)) +
    geom_line(size = 1) +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16)) +
    ylab("Density") +
    xlab("Value of Y") +
    scale_color_manual(values = c("blue", "red")) +
    theme(legend.position = c(0.8, 0.95), legend.title = element_blank())
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{MAR Example}

<<>>=
## Simulate MAR Missingness:
mVec <- x0 < quantile(x0, probs = pm)
mean(mVec)

yMar       <- y0
yMar[mVec] <- NA

cor(yMar, x0, use = "pairwise") # Not looking so good :(
@

\pagebreak

<<echo = FALSE, out.width = "65%">>=
yDen <- density(yMar, na.rm = TRUE)

pDat <- data.frame(y = c(y0Den$y, yDen$y),
                   x = c(y0Den$x, yDen$x),
                   g = rep(c("Complete", "MAR w/ Deletion"),
                           each = length(yDen$y)
                           )
                   )

marP <- ggplot(data = pDat, mapping = aes(x = x, y = y, color = g)) +
    geom_line(size = 1) +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16)) +
    ylab("Density") +
    xlab("Value of Y") +
    scale_color_manual(values = c("blue", "red")) +
    theme(legend.position = c(0.8, 0.95), legend.title = element_blank())
marP
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{MNAR Example}

<<>>=
## Simulate MNAR Missingness:
mVec <- y0 < quantile(y0, probs = pm)
mean(mVec)

yMnar       <- y0
yMnar[mVec] <- NA

cor(yMnar, x0, use = "pairwise") # Hmm...looks pretty bad.
@

\pagebreak

<<echo = FALSE, out.width = "65%">>=
yDen <- density(yMnar, na.rm = TRUE)

pDat <- data.frame(y = c(y0Den$y, yDen$y),
                   x = c(y0Den$x, yDen$x),
                   g = rep(c("Complete", "MNAR w/ Deletion"),
                           each = length(yDen$y)
                           )
                   )

ggplot(data = pDat, mapping = aes(x = x, y = y, color = g)) +
    geom_line(size = 1) +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16)) +
    ylab("Density") +
    xlab("Value of Y") +
    scale_color_manual(values = c("blue", "red")) +
    theme(legend.position = c(0.8, 0.95), legend.title = element_blank())
@

\end{frame}

%------------------------------------------------------------------------------%
\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Crucial Nuance}

  In our previous MAR example, ignoring the predictor of missingness actually
  produces \emph{Indirect MNAR}.\\

  \pause
  \va

  \rmsc{Question:} What happens if we ignore the predictor of missingness, but
  that predictor is independent of our study variables?

  \pause

<<>>=
mVec <- z0 < quantile(z0, probs = pm)

y       <- y0
y[mVec] <- NA

cor(y, x0, use = "pairwise")
@

\rmsc{Answer:} We get back to MCAR :)

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Crucial Nuance}

  The missing data mechanisms are not simply characteristics of an incomplete
  dataset; we also need to account for the analysis.
  \vb
  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, out.width = "100%", message = FALSE>>=
marP + scale_color_manual(values = c("blue", "red"),
                          labels = c("Complete", "Indirect MNAR")
                          )
@

\end{column}
\begin{column}{0.5\textwidth}

<<echo = FALSE, out.width = "100%">>=
yDen <- density(y, na.rm = TRUE)

pDat <- data.frame(y = c(y0Den$y, yDen$y),
                   x = c(y0Den$x, yDen$x),
                   g = rep(c("Complete", "MCAR2"),
                           each = length(yDen$y)
                           )
                   )

ggplot(data = pDat, mapping = aes(x = x, y = y, color = g)) +
    geom_line(size = 1) +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16)) +
    ylab("Density") +
    xlab("Value of Y") +
    scale_color_manual(values = c("blue", "red")) +
    theme(legend.position = c(0.8, 0.95), legend.title = element_blank())
@

\end{column}
\end{columns}

\end{frame}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Effects of Deletion}

  As we saw in the preceding plots, excluding incomplete cases usually alters
  the variables' distributions.
  \vc
  \begin{itemize}
  \item The statistics upon which we base our analyses generally summarize these 
    distributions.
    \vc
  \item Problems with the distributions show up as bias in the results of our 
    anlayses.
  \end{itemize}
  
<<>>=
diabetes1 <- diabetes2 <- readRDS("../data/diabetes.rds")

mVec <- simLogisticMissingness0(data    = diabetes1,
                                pm      = 0.3,
                                preds   = c("bmi", "bp"),
                                stdData = TRUE)$r
diabetes2[mVec, "glu"] <- NA
@ 

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Example}
 
<<echo = FALSE, cache = TRUE, fig.asp = 0.55>>=
diabetes1$Response <- factor(mVec, labels = c("Observed", "Missing"))

p0 <- ggplot(data = diabetes1, mapping = aes(x = bmi, y = glu)) +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16)) +
    xlab("BMI") +
    ylab("Blood Glucose")

ggarrange(
    p0 + 
    geom_point(size = 0.75) + 
    ggtitle("Fully Observed Data") +
    theme(text = element_text(size = 8),
          axis.title = element_blank(),
          plot.title = element_text(face = "bold", hjust = 0.5)
          ),
    
    p0 + 
    geom_point(aes(color = Response), size = 0.75) + 
    scale_color_manual(values = c("black", "gray")) +
    ggtitle("Incomplete Data") +
    theme(text = element_text(size = 8),
          axis.title = element_blank(),
          axis.text.y = element_blank(),
          plot.title = element_text(face = "bold", hjust = 0.5),
          legend.position = "none"),
    
    ncol = 2,
    widths = c(1.05, 1)
) %>%
    annotate_figure(left = text_grob("Blood Glucose", 
                                     family = "Courier", 
                                     size = 10,
                                     rot = 90),
                    bottom = text_grob("BMI", family = "Courier", size = 10)
                    )
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<>>=
diabetes1 %>% select(bmi, glu, bp) %>% cor()
diabetes2 %>% select(bmi, glu, bp) %>% cor(use = "complete")
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<>>=
mean(diabetes1$glu)
mean(diabetes2$glu, na.rm = TRUE)
var(diabetes1$glu)
var(diabetes2$glu, na.rm = TRUE)
@ 

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Good Methods}

  Multiple Imputation (MI)
  \vc
  \begin{itemize}
  \item Replace the missing values with $M$ plausible estimates
    \vc
    \begin{itemize}
    \item Essentially, a repeated application of stochastic regression
      imputation (with a particular type of regression model)
      \vc
    \item Produces unbiased parameter estimates and predictions
      \vc
    \item Produces ``correct'' standard errors, CIs, and prediction intervals
      \vc
    \item Very, very flexible
      \vc
    \item Computationally expensive
    \end{itemize}
  \end{itemize}
  
  \pagebreak
  
  Full Information Maximum Likelihood (FIML)
  \vc
  \begin{itemize}
  \item Adjust the objective function to only consider the observed parts of the
    data
    \vc
    \begin{itemize}
    \item Models are directly estimated in the presence of missing data
      \vc
    \item The predictors of nonresponse must be included in the model, somehow
      \vc
    \item Unless you write your own optimization program, FIML is only available
      for certain types of models
      \vc
    \item In linear regression models, FIML cannot treat missing data on
      predictors (if the predictors are taken as fixed)
    \end{itemize}
  \end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[allowframebreaks, fragile]{Good Methods}

  What happens when we apply MI to our previous MAR example?
<<>>=
## Estimate imputation model:
miceOut <- mice(data      = data.frame(y = yMar, x = x0),
                m         = 25,
                maxit     = 1,
                method    = "norm",
                printFlag = FALSE)

## Estimate and pool M correlations:
with(miceOut, cor(y, x))$analyses %>% unlist() %>% mean()
@

The MI-based parameter estimate looks good.
\begin{itemize}
\item MI produces unbiased estimates of the parameter when data are MAR.
\end{itemize}

\pagebreak

<<echo = FALSE, out.width = "65%">>=
impX <- impY <- list()
for(m in 1 : miceOut$m) {
    tmp       <- density(complete(miceOut, m)$y)
    impX[[m]] <- tmp$x
    impY[[m]] <- tmp$y
}

pDat <- data.frame(x = unlist(impX),
                   y = unlist(impY),
                   g = rep(1 : miceOut$m, each = length(impX[[1]])),
                   c = "MAR w/ MI")

pDat <- rbind.data.frame(pDat,
                         data.frame(x = yDen$x, 
                                    y = yDen$y, 
                                    g = miceOut$m + 1, 
                                    c = "Complete")
                         )

ggplot(data    = pDat, 
       mapping = aes(x = x, y = y, color = c, group = g, size = c)
       ) +
    geom_line() +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16)) +
    ylab("Density") +
    xlab("Value of Y") +
    scale_color_manual(values = c("black", "red")) +
    scale_size_manual(values = c(1, 0.5)) +
    theme(legend.position = c(0.8, 0.95), legend.title = element_blank())
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Good Methods}

  What about applying MI to our MNAR example?
<<>>=
## Estimate imputation model:
miceOut <- mice(data      = data.frame(y = yMnar, x = x0),
                m         = 25,
                maxit     = 1,
                method    = "norm",
                printFlag = FALSE)

## Estimate and pool M correlations:
with(miceOut, cor(y, x))$analyses %>% unlist() %>% mean()
@

The MI-based parameter estimate is still biased.
\begin{itemize}
\item MI cannot correct bias in parameter estimates when data are MNAR.
\end{itemize}

\pagebreak

<<echo = FALSE, out.width = "65%">>=
mnarDen <- density(yMnar, na.rm = TRUE)

impX <- impY <- list()
for(m in 1 : miceOut$m) {
    tmp       <- density(complete(miceOut, m)$y)
    impX[[m]] <- tmp$x
    impY[[m]] <- tmp$y
}

pDat <- data.frame(x = unlist(impX),
                   y = unlist(impY),
                   g = rep(1:miceOut$m, each = length(impX[[1]])),
                   c = "MNAR w/ MI")

pDat <- rbind.data.frame(pDat,
                         data.frame(x = mnarDen$x, 
                                    y = mnarDen$y, 
                                    g = miceOut$m + 1, 
                                    c = "MNAR w/ Deletion"),
                         data.frame(x = yDen$x, 
                                    y = yDen$y, 
                                    g = miceOut$m + 2,
                                    c = "Complete")
                         )

ggplot(data = pDat, mapping = aes(x = x, y = y, color = c, group = g, size = c)) +
    geom_line() +
    theme_classic() +
    theme(text = element_text(family = "Courier", size = 16)) +
    ylab("Density") +
    xlab("Value of Y") +
    scale_color_manual(values = c("black", "blue", "red")) +
    scale_size_manual(values = c(1, 1, 0.5)) +
    theme(legend.position = c(0.8, 0.95), legend.title = element_blank())
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}[fragile]{MI Example}
 
<<>>=
## The mice package does MI:
library(mice)

## Multiply impute the missing data:
miceOut <- mice(data      = diabetes2, 
                m         = 25, 
                maxit     = 1,
                printFlag = FALSE,
                method    = "norm")
@ 

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{MI Example}

<<>>=
## Complete data:
diabetes1 %>% select(bmi, glu, bp) %>% cor()

## MI:
pooledCorMat(miceOut, c("bmi", "glu", "bp"))
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{MI Example}

<<>>=
mean(diabetes1$glu)
with(miceOut, mean(glu))$analyses %>% unlist() %>% mean()
var(diabetes1$glu)
with(miceOut, var(glu))$analyses %>% unlist() %>% mean()
@ 

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{FIML Example}
  
<<>>=
fit <- diabetes2 %>% 
    select(bmi, glu, bp) %>% 
    lavCor(missing = "fiml", output = "sampstat")

## Complete data:
diabetes1 %>% summarize(mean = mean(glu), var = var(glu))

## FIML:
fit %$% c(mean = mean[["glu"]], var = cov["glu", "glu"])
@   

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{FIML Example}

<<>>=
## Complete data:
diabetes1 %>% select(bmi, glu, bp) %>% cor() %>% round(3)

## FIML:
fit$cov %>% cov2cor()
@

\end{frame}

%------------------------------------------------------------------------------%

\sectionslide{Imputation-Based Solutions}

%------------------------------------------------------------------------------%

\begin{frame}{Prediction Example}
  
  To fix ideas, let's consider the \emph{diabetes} data and the following model:
  \begin{align*}
    Y_{LDL} = \beta_0 + \beta_1 X_{BP} + \beta_2 X_{gluc} + \beta_3 X_{BMI} + 
    \varepsilon
  \end{align*}
  
<<echo = FALSE>>=
diabetes <- readRDS(paste0(dataDir, "diabetes.rds"))

trainDat <- diabetes[1 : 400, ]
testDat  <- diabetes[401 : 442, ]

out1 <- lm(ldl ~ bp + glu + bmi, data = trainDat)

b0 <- round(coef(out1)[1], 3)
b1 <- round(coef(out1)[2], 3)
b2 <- round(coef(out1)[3], 3)
b3 <- round(coef(out1)[4], 3)

x1 <- testDat[1, "bp"]
x2 <- testDat[1, "glu"]
x3 <- testDat[1, "bmi"]

confInt <- round(
    predict(out1, newdat = testDat, interval = "confidence")[1, 2 : 3], 3
)
predInt <- round(
    predict(out1, newdat = testDat, interval = "prediction")[1, 2 : 3], 3
)
@ 

Training this model on the first $N = 400$ patients' data produces the following
fitted model:
\begin{align*}
  Y_{LDL} = \Sexpr{b0} + \Sexpr{b1} X_{BP} + \Sexpr{b2} X_{gluc} + 
  \Sexpr{b3} X_{BMI}
\end{align*}
\pause
Suppose a new patient presents with $BP = \Sexpr{x1}$, $gluc = \Sexpr{x2}$, and
$BMI = \Sexpr{x3}$. We can predict their $LDL$ score by:
\begin{align*}
  \hat{Y}_{LDL} &= \Sexpr{b0} + \Sexpr{b1} (\Sexpr{x1}) + \Sexpr{b2} 
  (\Sexpr{x2}) + \Sexpr{b3} (\Sexpr{x3})\\
  &= \Sexpr{round(predict(out1, testDat[1 : 2, ])[1], 3)}
\end{align*}

\end{frame}

%------------------------------------------------------------------------------%

\subsection{Single Imputation}

%------------------------------------------------------------------------------%

\begin{frame}{Imputation is Just Prediction*}
  
  In Lecture 3, you heard a bit about missing data imputation.
  \begin{itemize}
  \item Multiple imputation is one of the best ways to treat missing data.
  \end{itemize}
  \vb
  Imputation is nothing more than a type of prediction.
  \begin{enumerate}
  \item Train a model on the observed parts of the data, $Y_{obs}$.
    \begin{itemize}
    \item Train the imputation model.
    \end{itemize}
  \item Predict the missing values, $Y_{mis}$.
    \begin{itemize}
    \item Generate imputations.
    \end{itemize}
  \item Replace the missing values with these predictions.
    \begin{itemize}
    \item Impute the missing data.
    \end{itemize}
  \end{enumerate}
  \vb
  Imputation can be used to support either prediction or inference.
  \begin{itemize}
  \item Our goals will dictate what type of imputation we need to do.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{*Levels of Uncertainty Modeling}

  \citet{vanBuuren:2018} provides a very useful classification of different 
  imputation methods:
  \vb
  \begin{enumerate}
  \item Simple Prediction
    \begin{itemize}
    \item The missing data are naively filled with predicted values from some 
      regression equation.
    \item All uncertainty is ignored.
    \end{itemize}
    \vb
  \item Prediction + Noise
    \begin{itemize}
    \item A random residual error is added to each predicted value to create the 
      imputations.
    \item Only uncertainty in the predicted values is modeled.
    \item The imputation model itself is assumed to be correct and error-free.
    \end{itemize}
    \vb
  \item Prediction + Noise + Model Error
    \begin{itemize}
    \item Uncertainty in the imputation model itself is also modeled.
    \item Only way to get fully proper imputations in the sense of 
      \citet{rubin:1987}.
    \end{itemize}
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Do we really need to worry?}

  The arguments against single imputation can seem archaic and petty. Do we 
  really need to worry about this stuff?\\  
  \pause
  \vc
  \begin{itemize}
  \item YES!!! (At least if you care about inference)\\
  \end{itemize}
  \vb
  The following are results from a simple Monte Carlo simulation:
  
<<echo = FALSE, results = "asis">>=
simRes <- readRDS(paste0(dataDir, "simResMat.rds"))

simResTab <- 
    xtable(simRes, 
           caption = "Mean Correlation Coefficients and Type I Error Rates",
           digits  = 3,
           align   = c("r", rep("c", 4))
           )

print(simResTab, scale = 0.8, booktabs = TRUE)
@ 

\pause
\vx{-12}
\begin{itemize}
\item Conditional mean substitution overestimates the correlation effect.
  \vc
\item Both single imputation methods inflate Type I error rates.
  \vc
\item MI provides unbiased point estimates and accurate Type I error rates.
\end{itemize}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Simulate Some Toy Data}
  
<<>>=
nObs <- 1000 # Sample Size
pm   <- 0.3  # Proportion Missing

sigma <- matrix(c(1.0, 0.5, 0.0,
                  0.5, 1.0, 0.3,
                  0.0, 0.3, 1.0),
                ncol = 3)

dat0 <- as.data.frame(rmvnorm(nObs, c(0, 0, 0), sigma))
colnames(dat0) <- c("y", "x", "z")
@ 

\pagebreak

<<>>=
## Impose MAR Nonresponse:
dat1 <- dat0
mVec <- with(dat1, x < quantile(x, probs = pm))

dat1[mVec, "y"] <- NA

## Subset the data:
yMis <- dat1[mVec, ]
yObs <- dat1[!mVec, ]
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Look at the Data}
  
\begin{columns}
\begin{column}{0.5\textwidth}
      
\begin{onlyenv}<1>
<<>>=
round(head(dat0, n = 5), 3)
@ 
\end{onlyenv}

\begin{onlyenv}<2>
<<>>=
round(head(dat1, n = 5), 3)
@
\end{onlyenv}
      
\end{column}
\begin{column}{0.5\textwidth}
      
\only<1>{
<<echo = FALSE, out.width = "90%">>=
with(dat0, gg0(x = x, y = y))
@
}
\only<2>{
<<echo = FALSE, out.width = "90%">>=
p1 <- with(yObs, gg0(x = x, y = y)) +
    geom_point(mapping = aes(x = x, y = y), data = dat0[mVec, ], color = "gray")
p1
@ 
}

\end{column}
\end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Expected Imputation Model Parameters}
   
\begin{columns}
\begin{column}{0.5\textwidth}

<<>>=
lsFit <- lm(y ~ x + z, data = yObs)

beta  <- coef(lsFit)
sigma <- summary(lsFit)$sigma

as.matrix(beta)
sigma
@ 

\end{column}
\begin{column}{0.5\textwidth}
    
<<echo = FALSE, out.width = "90%">>=
fit0 <- lm(y ~ x, data = yObs)
b0   <- coef(fit0)
s0   <- summary(fit0)$sigma

p2 <- p1 + 
    geom_abline(intercept = b0[1], slope = b0[2], color = "blue", lwd = 1)
p2
@ 
  
\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Conditional Mean Substitution}

\begin{columns}
\begin{column}{0.5\textwidth}

<<>>=
## Generate imputations:
imps <- beta[1] + 
    beta[2] * yMis[ , "x"] + 
    beta[3] * yMis[ , "z"]

## Fill missing cells in Y:
dat1[mVec, "y"] <- imps

round(head(dat1, n = 5), 3)
@ 

\end{column}
\begin{column}{0.5\textwidth}

<<echo = FALSE, out.width = "90%">>=
imps <- predict(fit0, newdata = yMis)
p2 + geom_point(mapping = aes(x = yMis[, "x"], y = imps),  color = "red")
@ 
  
\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Stochastic Regression Imputation}
   
\begin{columns}
\begin{column}{0.52\textwidth}
  
<<>>=
## Generate imputations:
imps <- imps + 
    rnorm(nrow(yMis), 0, sigma)

## Fill missing cells in Y:
dat1[mVec, "y"] <- imps

round(head(dat1, n = 5), 3)
@ 

\end{column}
\begin{column}{0.5\textwidth}
  
<<echo = FALSE, out.width = "90%">>=
imps <- imps + rnorm(length(imps), 0, s0)
p2 + geom_point(mapping = aes(x = yMis[, "x"], y = imps),  color = "red")
@ 
  
\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\subsection{Multiple Imputation}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Flavors of MI}
  
  MI simply repeats a single regression imputation $M$ times.
  \begin{itemize}
  \item The specifics of the underlying regression imputation are important.
  \end{itemize}
  \vb
  \pause
  Simply repeating the stochastic regression imputation procedure described 
  above won't suffice.
  \begin{itemize}
  \item Still produces too many Type I errors
  \end {itemize}
  
<<echo = FALSE, results = "asis">>=
simRes2 <- readRDS(paste0(dataDir, "simResMat2.rds"))

simResTab2 <- 
    xtable(simRes2, 
           caption = "Mean Correlation Coefficients and Type I Error Rates",
           digits  = 3,
           align   = c("r", rep("c", 3))
           )

print(simResTab2, scale = 0.8, booktabs = TRUE) 
@

\vx{-16}
\begin{itemize}
\item Type I error rates for PN-Type MI are much better than they were for 
  single stochastic regression imputation, but they're still too high.
\end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Proper MI}
  
  The problems on the previous slide arise from using the same regression 
  coefficients to create each of the $M$ imputations.
  \begin{itemize}
  \item Implies that you're using the ``correct'' coefficients.
    \vb
  \item This assumption is plainly ridiculous.
    \begin{itemize}
    \item If we don't know some values of our outcome variable, how can we know 
      the ``correct'' coefficients to link the incomplete outcome to the 
      observed predictors?
    \end{itemize}
    \vb
    \pause
  \item Proper MI also models uncertainty in the regression coefficients used to 
    create the imputations.
    \begin{itemize}
    \item A different set of of coefficients is randomly sampled (using Bayesian 
      simulation) to create each of the $M$ imputations.
      \vc
    \item The tricky part about implemented MI is deriving the distributions 
      from which to sample these coefficients.
    \end{itemize}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Setting Up Proper MI}
  
  Our imputation model is simply a linear regression model:
  \begin{align*}
    Y = \mathbf{X} \beta + \varepsilon
  \end{align*}
  To fully account for model uncertainty, we need to randomly sample both 
  $\beta$ and $\text{var}(\varepsilon) = \sigma^2$.
  \begin{itemize}
  \item \textsc{Question:} Why do we only sample $\sigma^2$ and not 
    $\varepsilon$?
  \end{itemize}
  \pause
  \va
  For a simple imputation model with a normally distributed outcome and 
  uninformative priors, we need to specify two distributions:
  \begin{enumerate}
    \item The marginal posterior distribution of $\sigma^2$
    \item The conditional posterior distribution of $\beta$ 
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Marginal Distribution of $\sigma^2$}
 
  We first specify the marginal posterior distribution for the noise variance,
  $\sigma^2$.
  \vc
  \begin{itemize}
  \item This distribution does not depend on any other parameters.
  \end{itemize}
  \begin{align}
    \sigma^2 &\sim \text{Inv-}\chi^2 \left(N - P, MSE \right) \label{sigma2PosteriorEq}\\
    &\text{with } MSE = \frac{1}{N - P} \left( Y - \mathbf{X}\hat{\beta}_{ls} \right)^T \left( Y - \mathbf{X}\hat{\beta}_{ls} \right) \notag
  \end{align}
  \vx{-12}
  \begin{itemize}
  \item $\sigma^2$ follows a scaled inverse $\chi^2$ distribution.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Conditional Distribution of $\beta$}

  We then specify the conditional posterior distribution for $\beta$. 
  \vc
  \begin{itemize}
  \item This distribution is conditioned on a specific value of $\sigma^2$.
  \end{itemize}
  \begin{align}
    \beta \sim \text{MVN} \left( \hat{\beta}_{ls}, ~ \sigma^2 (\mathbf{X}^T \mathbf{X})^{-1} \right) \label{betaPosteriorEq}
  \end{align}
  \vx{-12}
  \begin{itemize}
  \item $\beta$ (conditionally) follows a multivariate normal distribution.
  \end{itemize}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{PPD of the Missing Data}
  
  Once we've sampled our imputation model parameters, we can construct the 
  posterior predictive distribution of the missing data.
  \vc
  \begin{itemize}
  \item This is the distribution from which we sample our imputed values.
    \vc
  \item In practice, we directly compute the imputations based on the simulated 
    imputation model parameters.
  \end{itemize}
  \begin{align}
    Y_{imp} &= \mathbf{X}_{mis}\tilde{\beta} + \tilde{\varepsilon} \label{impPosteriorEq}\\
    &\text{with } \varepsilon \sim \text{N} \left( 0, \widetilde{\sigma^2} \right) \notag
  \end{align}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{General Steps for Basic MI}
  
  With all of the elements in place, we can execute a basic MI by following 
  these steps:
  \vb
  \begin{enumerate}
  \item Find the least squares estimates of $\beta$, $\hat{\beta}_{ls}$, by 
    regressing the observed portion of $Y$ onto the the analogous rows of 
    $\mathbf{X}$.
    \vb
  \item Use $\hat{\beta}_{ls}$ to parameterize the posterior distribution of 
    $\sigma^2$, given by Equation \ref{sigma2PosteriorEq}, and draw $M$ samples 
    of $\sigma^2$ from this distribution.
    \vb
  \item For each of the $\sigma^2_m$, sample a corresponding value of $\beta$ 
    from Equation \ref{betaPosteriorEq}.
    \vb
  \item Plug the $M$ samples of $\beta$ and $\sigma^2$ into Equation 
    \ref{impPosteriorEq} to create the $M$ imputations.
  \end{enumerate}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Visualizing MI}
  
<<echo = FALSE, cache = TRUE>>=
## Estimate the imputation model:
nSams <- 100
preds <- cbind(1, yObs$x)
df    <- nrow(yObs) - length(b0)

sigmaScale <- (1 / df) * crossprod(yObs$y - preds %*% b0)
sigmaSams  <- rinvchisq(nSams, df = df, scale = sigmaScale)

betaVarDenom <- solve(crossprod(preds))

betaSams2 <- matrix(NA, nSams, length(b0))
for(m in 1 : nSams) {
    betaSigma <- sigmaSams[m] * betaVarDenom
    betaSams2[m, ] <- rmvnorm(1, mean = b0, sigma = betaSigma)
}

## Sample parameters:
index  <- sample((nSams / 2) : nSams, 3)
betas  <- betaSams2[index, ]
sigmas <- sigmaSams[index]

## Estimate posterior densities:
dS  <- density(sigmaSams)
dB1 <- density(betaSams2[ , 2])
@ 

Use Bayesian simulation to estimate posterior distributions for the imputation 
model parameters:\\

\vb

\begin{columns}
\begin{column}{0.5\textwidth}

<<echo = FALSE, out.width = "90%">>=
gg0(x = dS$x, y = dS$y, points = FALSE) + 
    geom_line() + 
    xlab("Residual Variance") + 
    ylab("Density")
@ 

\end{column}
\begin{column}{0.5\textwidth}
  
<<echo = FALSE, out.width = "90%">>=
gg0(x = dB1$x, y = dB1$y, points = FALSE) + 
    geom_line() + 
    xlab("Slope") + 
    ylab("Density")
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Visualizing MI}
  
\begin{columns}
\begin{column}{0.5\textwidth}

  \only<1>{
    Recall the incomplete data from the single imputation examples.
  }
  
  \only<2>{
    Sample values of $\beta_0$ and $\beta_1$:
    \begin{itemize}
    \item $\beta_0 = \Sexpr{round(betas[1, 1], 3)}$
    \item $\beta_1 = \Sexpr{round(betas[1, 2], 3)}$
    \end{itemize}
    \vb
    Define the predicted best-fit line:\\ 
    $\hat{Y}_{mis} = \Sexpr{round(betas[1, 1], 3)} + \Sexpr{round(betas[1, 2], 3)}X_{mis}$
  }
  
  \only<3>{
    Sample a value of $\sigma^2$:
    \begin{itemize}
    \item $\sigma^2 = \Sexpr{round(sigmas[1], 3)}$
    \end{itemize}
    \vb
    Generate imputations using the same procedure described in Single Stochastic 
    Regression Imputation:
    \begin{align*}
      Y_{imp} &= \hat{Y}_{mis} + \varepsilon\\
      \varepsilon &\sim \text{N}(0, \Sexpr{round(sigmas[1], 3)})
    \end{align*}
  }
  
  \only<4>{
    Sample values of $\beta_0$ and $\beta_1$:
    \begin{itemize}
    \item $\beta_0 = \Sexpr{round(betas[2, 1], 3)}$
    \item $\beta_1 = \Sexpr{round(betas[2, 2], 3)}$
    \end{itemize}
    \vb
    Define the predicted best-fit line:\\ 
    $\hat{Y}_{mis} = \Sexpr{round(betas[2, 1], 3)} + \Sexpr{round(betas[2, 2], 3)}X_{mis}$
  }
  
  \only<5>{
    Sample a value of $\sigma^2$:
    \begin{itemize}
    \item $\sigma^2 = \Sexpr{round(sigmas[2], 3)}$
    \end{itemize}
    \vb
    Generate imputations using the same procedure described in Single Stochastic 
    Regression Imputation:
    \begin{align*}
      Y_{imp} &= \hat{Y}_{mis} + \varepsilon\\
      \varepsilon &\sim \text{N}(0, \Sexpr{round(sigmas[2], 3)})
    \end{align*}
  }
  
  \only<6>{
    Sample values of $\beta_0$ and $\beta_1$:
    \begin{itemize}
    \item $\beta_0 = \Sexpr{round(betas[3, 1], 3)}$
    \item $\beta_1 = \Sexpr{round(betas[3, 2], 3)}$
    \end{itemize}
    \vb
    Define the predicted best-fit line:\\ 
    $\hat{Y}_{mis} = \Sexpr{round(betas[3, 1], 3)} + \Sexpr{round(betas[3, 2], 3)}X_{mis}$
  }
  
  \only<7>{
    Sample a value of $\sigma^2$:
    \begin{itemize}
    \item $\sigma^2 = \Sexpr{round(sigmas[3], 3)}$
    \end{itemize}
    \vb
    Generate imputations using the same procedure described in Single Stochastic 
    Regression Imputation:
    \begin{align*}
      Y_{imp} &= \hat{Y}_{mis} + \varepsilon\\
      \varepsilon &\sim \text{N}(0, \Sexpr{round(sigmas[3], 3)})
    \end{align*}
  }
  
\end{column}
\begin{column}{0.5\textwidth}
  
\only<1>{
<<echo = FALSE, out.width = "90%">>=
print(p1)
@
}
\only<2>{
<<echo = FALSE, out.width = "90%">>=
p3 <- p1 + 
    geom_abline(intercept = betas[1, 1], 
                slope     = betas[1, 2], 
                color     = "blue", 
                lwd       = 1)
p3
@
}
\only<3>{
<<echo = FALSE, out.width = "90%">>=
mPreds <- cbind(1, yMis$x)
imps   <- mPreds %*% betas[1, ] + rnorm(nrow(yMis), 0, sqrt(sigmas[1]))

p3 + geom_point(mapping = aes(x = yMis$x, y = imps), color = "red")
@
}
\only<4>{
<<echo = FALSE, out.width = "90%">>=
p3 <- p1 + 
    geom_abline(intercept = betas[1, 1], 
                slope     = betas[1, 2], 
                color     = "lightblue", 
                lwd       = 1) + 
    geom_abline(intercept = betas[2, 1], 
                slope     = betas[2, 2], 
                color     = "blue", 
                lwd       = 1)
p3
@
}
\only<5>{
<<echo = FALSE, out.width = "90%">>=
mPreds <- cbind(1, yMis$x)
imps   <- mPreds %*% betas[2, ] + rnorm(nrow(yMis), 0, sqrt(sigmas[2]))

p3 + geom_point(mapping = aes(x = yMis$x, y = imps), color = "red")
@
}
\only<6>{
<<echo = FALSE, out.width = "90%">>=
p3 <- p1 + 
    geom_abline(intercept = betas[2, 1], 
                slope     = betas[2, 2], 
                color     = "lightblue", 
                lwd       = 1) + 
    geom_abline(intercept = betas[3, 1], 
                slope     = betas[3, 2], 
                color     = "blue", 
                lwd       = 1)
p3

@
}
\only<7>{
<<echo = FALSE, out.width = "90%">>=
mPreds <- cbind(1, yMis$x)
imps   <- mPreds %*% betas[3, ] + rnorm(nrow(yMis), 0, sqrt(sigmas[3]))

p3 + geom_point(mapping = aes(x = yMis$x, y = imps), color = "red")
@
}

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<cache = TRUE>>=
## Impute the missing data 10 times:
miceOut <- mice(data   = bfi, 
                m      = 10, 
                maxit  = 10, 
                method = "pmm", 
                seed   = 235711, 
                print  = FALSE)

## Check convergence via PSR factor:
rhat(miceOut)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<out.width = "55%">>=
ggmice::plot_trace(miceOut)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<cache = TRUE>>=
## Extend the Markov chains by 10 interations:
miceOut <- mice.mids(miceOut, maxit = 10, printFlag = FALSE)

## Check convergence via PSR factor:
rhat(miceOut)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<out.width = "55%">>=
ggmice::plot_trace(miceOut)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<out.width = "55%">>=
mice::densityplot(miceOut)
@ 

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\subsection{MI-Based Analysis}

%------------------------------------------------------------------------------%

\begin{frame}{Doing MI-Based Analysis}
  
  An MI-based data analysis consists of three phases:
  \vb
  \begin{enumerate}
  \item The imputation phase \label{iStep}
    \begin{itemize}
    \item Replace missing values with $M$ plausible estimates.
    \item Produce $M$ completed datasets.
    \end{itemize}
    \vb
  \item The analysis phase \label{aStep}
    \begin{itemize}
    \item Estimate $M$ replicates of your analysis model.
    \item Fit the same model to each of the $M$ datasets from Step \ref{iStep}.
    \end{itemize}
    \vb
  \item The pooling phase
    \begin{itemize}
    \item Combine the $M$ sets of parameter estimates and standard errors from 
      Step \ref{aStep} into a single set of MI estimates.
    \item Use these pooled parameter estimates and standard errors for 
      inference.
    \end{itemize}
  \end{enumerate}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{MI-Based Analysis}

  \begin{center}
    \includegraphics<1>[width = 0.75\textwidth]{figures/mi_schematic0.pdf}
    \includegraphics<2>[width = 0.75\textwidth]{figures/mi_schematic1.pdf}
    \includegraphics<3>[width = 0.75\textwidth]{figures/mi_schematic2.pdf}
    \includegraphics<4>[width = 0.75\textwidth]{figures/mi_schematic.pdf}
  \end{center}
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Pooling MI Estimates}
  
  \citet{rubin:1987} formulated a simple set of pooling rules for MI estimates.
  \vb
  \begin{itemize}
  \item The MI point estimate of some interesting quantity, $Q^*$, is simply 
    the mean of the $M$ estimates, $\{\hat{Q}_m\}$:
    \begin{align*}
      Q^* &= \frac{1}{M} \sum_{m = 1}^M \hat{Q}_m\\
    \end{align*}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Pooling MI Estimates}
  
  The MI variability estimate, $T$, is a slightly more complex entity.
  \vb
  \begin{itemize}
  \item A weighted sum of the \emph{within-imputation} variance, $W$, and the 
    \emph{between-imputation} variance, $B$.
    \begin{align*}
      W &= \frac{1}{M} \sum_{m = 1}^M \widehat{SE}_{Q,m}^2\\
      B &= \frac{1}{M - 1} \sum_{m = 1}^M \left( \hat{Q}_m - Q^* \right)^2\\
      T &= W + \left( 1 + M^{-1} \right) B\\ 
      &= W + B + \frac{B}{M}
    \end{align*}
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Inference with MI Estimates}
  
  After computing $Q^*$ and $T$, we combine them in the usual way to get test 
  statistics and confidence intervals.
  \begin{align*}
    t &= \frac{Q^* - Q_0}{\sqrt{T}}\\
    CI &= Q^* \pm t_{crit} \sqrt{T}
  \end{align*}
  
  We must take care with our \emph{df}, though.
  \begin{align*}
    df = (M - 1) \left[1 + \frac{W}{\left(1 + M^{-1}\right)B}\right]^2
  \end{align*}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Fraction of Missing Information}
  
  In Lecture 4, we briefly discussed a very desirable measure of nonresponse: 
  \emph{fraction of missing information} (FMI).
  
  \begin{align*}
    FMI = \frac{r + \frac{2}{(df + 3)}}{r + 1} \approx \frac{(1 + M^{-1})B}{(1 + M^{-1})B + W} \rightarrow \frac{B}{B + W}
  \end{align*}
  where
  \begin{align*}
    r = \frac{(1 + M^{-1})B}{W}
  \end{align*}
  The FMI gives us a sense of how much the missing data (and their treatment) 
  have influence our parameter estimates.
  \vc
  \begin{itemize}
  \item We should report the FMI for an estimated parameter along with other 
     statistics (e.g., t-tests, p-values, effect sizes, etc.).
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{Example: Analysis \& Pooling}
  
  Analyze the multiply imputed datasets and pool results:
  
<<cache = TRUE>>=
library(semTools)

## semTools::cfa.mi() will do both the analysis and pooling phases:
miOut <- 
    cfa.mi(cfaMod, data = complete(miceOut, "all"), std.lv = TRUE)  
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example: Analysis \& Pooling}

<<echo = FALSE>>=
partSummary(miOut, 1, fmi = TRUE, drops = c("df", "riv")) %>% quiet()
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example: Analysis \& Pooling}

<<echo = FALSE>>=
partSummary(miOut, 2:3, 1:17, fmi = TRUE, drops = c("df", "riv")) %>% quiet()
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Model Fit}
  
  We cannot simply average the $\chi^2$ statistics as we do the parameter
  estimates.
  \vc
  \begin{itemize}
  \item We need to do some fancy processing to get a correctly distributed
    fit statistic.
    \end{itemize}
  \vb
  Three different formulations have been proposed, and they are typically 
  designated $D_1$, $D_2$, and $D_3$.
  \vc
  \begin{enumerate}
    \item $D_1$
      \vc
    \item $D_2$
      \vc
    \item $D_3$
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Model Fit}
  
<<>>=
what <- c("chisq", "df", "pvalue", "fmi", "rmsea", "cfi", "tli")
fitMeasures(miOut, test = "D3")[what] %>% round(3)
fitMeasures(miOut, test = "D2")[what] %>% round(3)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Model Fit}

<<>>=
## Define a restricted model:
cfaMod2 <- paste(cfaMod, "agree ~~ 0 * open", sep = "\n")

## Estimate the restricted model and pool the results:
miOut2 <- 
    cfa.mi(cfaMod2, data = complete(miceOut, "all"), std.lv = TRUE)

## Test the constraint via nested model comparison:
anova(miOut2, miOut, test = "D3")
anova(miOut2, miOut, test = "D2")
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Model Fit}

<<>>=
## Define a model with some parameter labels:
cfaMod3 <- "
agree =~ l11 * A1 + l21 * A2 + l31 * A3 + l41 * A4 + l51 * A5
open  =~ l12 * O1 + l22 * O2 + l32 * O3 + l42 * O4 + l52 * O5
"

## Estimate the model and pool the results:
miOut3 <- 
    cfa.mi(cfaMod3, data = complete(miceOut, "all"), std.lv = TRUE)

## Define some constraints:
cons <- "
l11 == l12
l21 == l22
l31 == l32
l41 == l42
l51 == l52
"
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Model Fit}

<<>>=
## Test the constraints via multivariate Wald tests:
lavTestWald.mi(miOut3, constraints = cons, test = "D1")
lavTestWald.mi(miOut3, constraints = cons, test = "D2")
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Model Fit}

<<>>=
## Define a model with some parameter labels:
cfaMod4 <- paste(cfaMod3, cons, sep = "\n")

## Estimate the model and pool the results:
miOut4 <- 
    cfa.mi(cfaMod4, data = complete(miceOut, "all"), std.lv = TRUE)

anova(miOut3, miOut4, test = "D2")
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Model Fit}

<<>>=
## Define some syntax to allow cross-loadings from the "agree" 
## factor to the "open" items:
adds <- "agree =~ O1 + O2 + O3"

## Test the parameter relaxations via a score test:
stD2 <- lavTestScore.mi(miOut, add = adds, test = "D2")
stD1 <- lavTestScore.mi(miOut, add = adds, test = "D1")
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Model Fit}

<<>>=
stD2
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Model Fit}

<<>>=
stD1
@ 

\end{frame}

%------------------------------------------------------------------------------%

\comment{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Special Pooling Considerations}
  
  The \citet{rubin:1987} pooling rules only hold when the parameter of 
  interest, $Q$, follows an approximately normal sampling distribution.
  \vc
  \begin{itemize}
  \item For substantially non-normal parameters, we may want to transform before 
    pooling and back-transform the pooled estimate.
  \end{itemize}
  \vb
  The following table, reproduced from \citet{vanBuuren:2018}, shows some 
  recommended transformations.
  \vx{-6}
  \begin{center}
    \begin{tabular}{lll}
      \hline
      \bf{Statistic} & \bf{Transformation} & \bf{Source}\\
      \hline
      Correlation & Fisher's $z$ & \citet{schafer:1997}\\
      Odds ratio & Logarithm & \citet{agresti:2013}\\
      Relative risk & Logarithm & \citet{agresti:2013}\\
      Hazard ratio & Logarithm & \shortcites{marshallEtAl:2009}\citet{marshallEtAl:2009}\\
      $R^2$ & Fisher's $z$ on square root & \citet{harel:2009}\\
      Survival probabilities & Complementary log-log & \citet{marshallEtAl:2009}\\
      Survival distribution & Logarithm & \citet{marshallEtAl:2009}\\
      \hline
    \end{tabular}
  \end{center}
  
\end{frame}
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\watermarkon %-----------------------------------------------------------------%

\sectionslide{ML-Based Solutions}

%------------------------------------------------------------------------------%

\begin{frame}{FIML Intuition}

  FIML is an ML estimation method that is robust to ignorable nonresponse.
  \vc
  \begin{itemize}
  \item FIML partitions the missing information out of the likelihood function 
    so that the model is only estimated from the observed parts of the data.
  \end{itemize}
  \vb
  After a minor alteration to the likelihood function, FIML reduces to simple ML 
  estimation.
  \vc
  \begin{itemize}
  \item So, let's review ML estimation before moving forward.
  \end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\subsection{Maximum Likelihood Estimation}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Maximum Likelihood Estimation}

  ML estimation simply finds the parameter values that are ``most likely'' to 
  have given rise to the observed data.
  \vb
  \begin{itemize}
  \item The \emph{likelihood} function is just a probability density (or mass) 
    function with the data treated as fixed and the parameters treated as 
    random variables.
    \vb
  \item Having such a framework allows us to ask: ``Given that I've observed 
    these data values, what parameter values most probably describe these 
    data?''
  \end{itemize}
  
  \pagebreak
  
  ML estimation is usually employed when there is no closed form solution for 
  the parameters we seek.
  \vb
  \begin{itemize}
  \item This is why you don't usually see ML used to fit general linear models.
  \end{itemize}
  \vb
  After choosing a likelihood function, we iteratively optimize the function to 
  produce the ML estimated parameters.
  \vb
  \begin{itemize}
  \item In practice, we nearly always work with the natural logarithm of the 
    likelihood function (i.e., the \emph{loglikelihood}).
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Likelihoods}
  
  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      Suppose we have the following model:
      \begin{align*}
        Y \sim \text{N}\left( \mu, \sigma^2 \right).
      \end{align*}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE, cache = TRUE>>=
x    <- seq(0, 15, length.out = 1000)
dat1 <- data.frame(x = x, y = dnorm(x, 7.5, 1.75))

ggplot(data = dat1, aes(x = x, y = y)) + 
    theme_classic() +
    geom_line() +
    theme(text       = element_text(size = 16, family = "Courier"),
          plot.title = element_text(size = 20, face = "bold", hjust = 0.5)
          ) +
    ylab("Density") +
    xlab("Y")
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Likelihoods}
  
  For a given $Y_n$, we have:
  \begin{align}
    P \left( Y_n|\mu, \sigma^2 \right) = 
    \frac{1}{\sqrt{2 \pi \sigma^2}} 
    e^{-\frac{\left( Y_n - \mu \right)^2}{2\sigma^2}}. \label{margPdf}
  \end{align}
  
  If we plug estimated parameters into Equation \ref{margPdf}, we get the 
  probability of observing $Y_n$ given $\hat{\mu}$ and $\hat{\sigma}^2$:
  \begin{align}
    P \left( Y_n|\hat{\mu}, \hat{\sigma}^2 \right) = 
    \frac{1}{\sqrt{2 \pi \hat{\sigma}^2}} 
    e^{-\frac{\left( Y_n - \hat{\mu}\right)^2}{2\hat{\sigma}^2}}. \label{estMargPdf}
  \end{align}
  
  Applying Equation \ref{estMargPdf} to all $N$ observations and multiplying the 
  results produces a \emph{likelihood}:
  \begin{align*}
    \hat{L} \left( \hat{\mu}, \hat{\sigma}^2 \right) = 
    \prod_{n = 1}^N P \left( Y_n|\hat{\mu}, \hat{\sigma}^2 \right).
  \end{align*}
  
\end{frame}

%%----------------------------------------------------------------------------%%

\begin{frame}{Likelihoods}
  
  We generally want to work with the natural logarithm of Equation 
  \ref{estMargPdf}. Doing so gives the \emph{loglikelihood}:
  \begin{align*}
  \hat{\mathcal{L}} \left( \hat{\mu}, \hat{\sigma}^2 \right) &= 
    \ln \prod_{n = 1}^N P \left( Y_n|\hat{\mu}, \hat{\sigma}^2 \right)\\ 
    &= -\frac{N}{2} \ln 2\pi - N \ln \hat{\sigma} - \frac{1}{2\hat{\sigma}^2} 
    \sum_{n = 1}^N \left( Y_n - \hat{\mu} \right)^2
  \end{align*}
  
  ML tries to find the values of $\hat{\mu}$ and $\hat{\sigma}^2$ that maximize 
  $\hat{\mathcal{L}} \left( \hat{\mu}, \hat{\sigma}^2 \right)$.
  \vc
  \begin{itemize}
  \item Find the values of $\hat{\mu}$ and $\hat{\sigma}^2$ that are \emph{most 
    likely}, given the observed values of $Y$.
  \end{itemize}
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}{Likelihoods}
  
    \begin{columns}
    \begin{column}{0.5\textwidth}
      
      Suppose we have a linear regression model:
      \begin{align*}
        Y &= \beta_0 + \beta_1 X + \varepsilon,\\[6pt]
        \varepsilon &\sim \text{N}\left( 0, \sigma^2 \right).
      \end{align*}
      This model can be equivalently written as:
      \begin{align*}
        Y \sim \text{N} \left( \beta_0 + \beta_1 X, \sigma^2 \right)
      \end{align*}
      
    \end{column}
    \begin{column}{0.5\textwidth}

      \begin{figure}
        \includegraphics[width = \textwidth]{figures/conditional_density_figure.png}\\
        \va
        \tiny{Image retrieved from:
          \url{http://www.seaturtle.org/mtn/archives/mtn122/mtn122p1.shtml}}
      \end{figure}
      
    \end{column}
    \end{columns}
    
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Likelihoods}
  
  For a given $\{Y_n, X_n\}$, we have:
  \begin{align}
    P \left( Y_n|X_n, \beta_0, \beta_1, \sigma^2 \right) = 
    \frac{1}{\sqrt{2 \pi \sigma^2}} 
    e^{-\frac{\left( Y_n - \beta_0 - \beta_1 X_n \right)^2}{2\sigma^2}}. \label{olsPdf}
  \end{align}
  
  If we plug our estimated parameters into Equation \ref{olsPdf}, we get the 
  probability of observing $Y_n$ given $\hat{Y}_n = \hat{\beta}_0 + 
  \hat{\beta}_1X_n$ and $\hat{\sigma}^2$.
  \begin{align}
    P \left( Y_n|X_n, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}^2 \right) = 
    \frac{1}{\sqrt{2 \pi \hat{\sigma}^2}} 
    e^{-\frac{\left( Y_n - \hat{\beta}_0 - \hat{\beta}_1 X_n \right)^2}{2\hat{\sigma}^2}} \label{estOlsPdf}
  \end{align}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Likelihoods}
 
  So, our final loglikelihood function would be the following:
  \begin{align*}
  \hat{\mathcal{L}} \left( \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}^2 \right) &= 
    \ln \prod_{n = 1}^N P \left( Y_n|X_n, \hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}^2 \right)\\ 
    &= -\frac{N}{2} \ln 2\pi - N \ln \hat{\sigma} - \frac{1}{2\hat{\sigma}^2} 
    \sum_{n = 1}^N \left( Y_n - \hat{\beta}_0 - \hat{\beta}_1 X_n \right)^2.
  \end{align*}
  
\end{frame}
  
\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}
    
<<echo = FALSE>>=
diabetes <- readRDS(paste0(dataDir, "diabetes.rds"))
@ 

<<>>=
## Fit a model:
out1 <- lm(ldl ~ bp + glu + bmi, data = diabetes)

## Extract the predicted values and estimated residual 
## standard error:
yHat <- predict(out1)
s    <- summary(out1)$sigma

## Compute the row-wise probabilities:
pY <- dnorm(diabetes$ldl, mean = yHat, sd = s)

## Compute the loglikelihood, and compare to R's version:
sum(log(pY)); logLik(out1)[1]
@

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\begin{frame}{Multivariate Normal Distribution}
  
  The PDF for the multivariate normal distribution is:
  \begin{align*}
    P(\mathbf{Y}|\mub, \Sigma) = 
    \frac{1}{\sqrt{(2\pi)^P|\Sigma|}} e^{-\frac{1}{2}(\mathbf{Y} - \mub)^T\Sigma^{-1}(\mathbf{Y} - \mub)}.
  \end{align*}
  So, the multivariate normal loglikelihood is:
  \begin{align*}
    \mathcal{L} \left( \mub, \Sigma \right) = 
    -\left[\frac{P}{2} \ln(2\pi) + \frac{1}{2} \ln |\Sigma| + \frac{1}{2} \right] \sum_{n = 1}^N(\mathbf{Y}_n - \mub)^T \Sigma^{-1}(\mathbf{Y}_n - \mub).
  \end{align*}
  Which can be further simplified if we multiply through by -2:
  \begin{align*}
    -2\mathcal{L} \left( \mub, \Sigma \right) = 
    \left[P \ln(2\pi) + \ln |\Sigma| \right] \sum_{n = 1}^N(\mathbf{Y}_n - \mub)^T \Sigma^{-1}(\mathbf{Y}_n - \mub).
  \end{align*}
  
\end{frame}

%%----------------------------------------------------------------------------%%

\begin{frame}{Steps of ML}

  \begin{enumerate}
  \item Choose a probability distribution, $f(Y|\theta)$, to describe the 
    distribution of the data, $Y$, given the parameters, $\theta$.
    \vc
  \item Choose some estimate of $\theta$, $\hat{\theta}^{(i)}$.
    \vc
  \item Compute each row's contribution to the loglikelihood function by 
    evaluating: $\ln \left[f\left(Y_n|\hat{\theta}^{(i)}\right)\right]$. 
    \label{rowContrib}
    \vc
  \item Sum the individual loglikelihood contributions from Step 
    \ref{rowContrib} to find the loglikelihood value, $\hat{\mathcal{L}}$. 
    \label{getLL}
    \vc
  \item Choose a ``better'' estimate of the parameters, $\hat{\theta}^{(i + 1)}$, 
    and repeat Steps \ref{rowContrib} and \ref{getLL}. \label{updateTheta}
    \vc
  \item Repeat Steps \ref{rowContrib} -- \ref{updateTheta} until the change 
    between $LL^{(i - 1)}$ and $LL^{(i)}$ falls below some trivially small 
    threshold.
    \vc
  \item Take $\hat{\theta}^{(i)}$ as your estimated parameters.
  \end{enumerate}

\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{ML Example}
  
  Recall the $n$th observation's contribution to the multivariate normal 
  loglikelihood function:
  \begin{align*}
   \mathcal{L} \left( \mub, \Sigma \right)_n = 
   -\frac{P}{2} \ln(2\pi) - \frac{1}{2} \ln |\Sigma| - \frac{1}{2} (\mathbf{Y}_n - \mub)^T \Sigma^{-1}(\mathbf{Y}_n - \mub).
  \end{align*}

  \va

  It turns out that this function is readily available in R via the 
  \pkg{mvtnorm} package:

<<eval = FALSE>>=
## Vector of row-wise contributions to the overall LL:
ll0 <- dmvnorm(y, mean = mu, sigma = sigma, log = TRUE)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{ML Example}

  We can wrap the preceding code in a nice R function:

<<>>=
## Complete data loglikelihood function:
ll <- function(par, data) {
    ## Extract the parameter matrices:
    p  <- ncol(data)
    mu <- par[1:p]
    
    ## Populate sigma from its Cholesky factor:
    sigma <- vecChol(tail(par, -p), p = p, revert = TRUE)
    
    ## Compute the row-wise contributions to the LL:
    ll0 <- dmvnorm(data, mean = mu, sigma = sigma, log = TRUE)
    
    sum(ll0)# return the overall LL value
}
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{ML Example}

  We'll also need the following helper function:
  
<<>>=
## Convert from covariance matrix to vectorized Cholesky factor and back:
vecChol <- function(x, p, revert = FALSE) {
    if(revert) {
        tmp                  <- matrix(0, p, p)
        tmp[!lower.tri(tmp)] <- x
        crossprod(tmp)
    }
    else
        chol(x)[!lower.tri(x)]
}
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{ML Example}
  
  The \pkg{optimx} package can numerically optimize arbitrary functions.
  \begin{itemize}
  \item We can use it to (semi)manually implement ML.
  \end{itemize}
  
<<>>=
## Subset the 'diabetes' data:
dat1 <- diabetes[ , c("bmi", "ldl", "glu")] %>% as.matrix()

## Choose some starting values:
m0   <- rep(0, 3)
s0   <- diag(3) %>% vecChol()
par0 <- c(m0, s0)

## Use optimx() to numerically optimize the LL function:
mle <- optimx(par     = par0,
              fn      = ll,
              data    = dat1,
              method  = "BFGS",
              control = list(maximize = TRUE, maxit = 1000)
              )
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{ML Example}
  
  Finally, let's check convergence and extract the optimized parameters:
  
<<>>=
## Check convergence:
mle[c("convcode", "kkt1", "kkt2")]

## Get the optimize mean vector and covariance matrix:
muHat    <- mle[1:3]
sigmaHat <- mle[4:9] %>% as.numeric() %>% vecChol(p = 3, revert = TRUE)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{ML Example}
  
<<echo = FALSE, results = "asis">>=
names(muHat) <- colnames(dat1)

muMat           <- rbind(muHat, colMeans(dat1))
rownames(muMat) <- c("ML", "Closed Form")

print(xtable(muMat, digits = 3, caption = "Estimated Means"), booktabs = TRUE)
@ 

\vx{-12}

\begin{columns}
  \begin{column}{0.5\textwidth}
    
<<echo = FALSE, results = "asis">>= 
colnames(sigmaHat) <- rownames(sigmaHat) <- colnames(dat1)
print(xtable(sigmaHat, digits = 3, caption = "ML Covariance Matrix"), 
      booktabs = TRUE)
@ 

\end{column}
\begin{column}{0.5\textwidth}
  
<<echo = FALSE, results = "asis">>= 
print(xtable(cov(dat1), digits = 3, caption = "Closed Form Covariance Matrix"), 
      booktabs = TRUE)
@

\end{column}
\end{columns}

\end{frame}

\watermarkon %-----------------------------------------------------------------%

\subsection{Full Information Maximum Likelihood}

%------------------------------------------------------------------------------%

\begin{frame}{From ML to FIML}

  The $n$th observation's contribution to the multivariate normal loglikelihood 
  function would be the following:
  \begin{align}
   \mathcal{L} \left( \mub, \Sigma \right)_n = 
   -\frac{P}{2} \ln(2\pi) - \frac{1}{2} \ln |\Sigma| - \frac{1}{2} (\mathbf{Y}_n - \mub)^T \Sigma^{-1}(\mathbf{Y}_n - \mub). \label{llContrib}
  \end{align}\\
  \va
  \pause
  FIML just tweaks Equation \ref{llContrib} a tiny bit: 
  \begin{align*}
    \mathcal{L} \left( \mub, \Sigma \right)_{fiml,n} = 
    -\frac{P}{2} \ln(2\pi) - \frac{1}{2} \ln |\Sigma_q| - \frac{1}{2} (\mathbf{Y}_n - \mub_q)^T \Sigma_q^{-1}(\mathbf{Y}_n - \mub_q).
  \end{align*}
  Where $q = 1, 2, \ldots, Q$ indexes response patterns.
  
\end{frame}

\watermarkoff %----------------------------------------------------------------%

\begin{frame}[fragile]{FIML Example}

  First things first, we need to punch some holes in our example data.
  
<<>>=
## Impose MAR missing:
dat2 <- imposeMissData(data    = dat1,
                       targets = c("ldl", "glu"),
                       preds   = "bmi",
                       pm      = 0.3,
                       types   = c("low", "high"),
                       stdDat  = TRUE)
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Visualize the Response Patterns}

<<echo = FALSE>>=
nPats <- md.pattern(dat2, plot = FALSE) %>% nrow() - 1
@

  \begin{columns}
    \begin{column}{0.5\textwidth}
      
      The data contain \Sexpr{nPats} unique response patterns.
      \vc
      \begin{itemize}
      \item We'll define \Sexpr{nPats} different version of $\mu$ and $\Sigma$.
        \vc
      \item We'll calculate each individual loglikelihood contributions using 
        the appropriate flavor of $\mu$ and $\Sigma$. 
      \end{itemize}
      
    \end{column}
    \begin{column}{0.5\textwidth}
      
<<echo = FALSE>>=
  tmp <- md.pattern(dat2)
@

    \end{column}
  \end{columns}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{FIML Example}

<<>>=
## Compute the within-pattern contributions to the LL:
ll0 <- function(i, mu, sigma, pats, ind, data) {
    ## Define the current response pattern:
    p1 <- pats[i, ]
    
    if(sum(p1) > 1) # More than one observed variable?
        dmvnorm(x     = data[ind == i, p1],
                mean  = mu[p1],
                sigma = sigma[p1, p1],
                log   = TRUE)
    else
        dnorm(x    = data[ind == i, p1],
              mean = mu[p1],
              sd   = sqrt(sigma[p1, p1]),
              log  = TRUE)
}
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{FIML Example}

<<>>=
## FIML loglikelihood function:
llm <- function(par, data, pats, ind) {
    ## Extract the parameter matrices:
    p  <- ncol(data)
    mu <- par[1:p]
    
    ## Populate sigma from its Cholesky factor:
    sigma <- vecChol(tail(par, -p), p = p, revert = TRUE)
    
    ## Compute the pattern-wise contributions to the LL:
    ll1 <- sapply(X     = 1:nrow(pats),
                  FUN   = ll0,
                  mu    = mu,
                  sigma = sigma,
                  pats  = pats,
                  ind   = ind,
                  data  = data)

    sum(unlist(ll1))
}
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{FIML Example}

<<>>=
## Summarize response patterns:
pats <- uniquecombs(!is.na(dat2))
ind  <- attr(pats, "index")

## Choose some starting values:
m0   <- colMeans(dat2, na.rm = TRUE)
s0   <- cov(dat2, use = "pairwise") %>% vecChol()
par0 <- c(m0, s0)

## Use optimx() to numerically optimize the LL function:
mle <- optimx(par     = par0,
              fn      = llm,
              data    = dat2,
              pats    = pats,
              ind     = ind,
              method  = "BFGS",
              control = list(maximize = TRUE, maxit = 1000)
              )
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{FIML Example}
  
  Check convergence and extract the optimized parameters:
  
<<>>=
## Check convergence:
mle[c("convcode", "kkt1", "kkt2")]

## Get the optimize mean vector and covariance matrix:
muHat1    <- mle[1:3]
sigmaHat1 <- mle[4:9] %>% as.numeric() %>% vecChol(p = 3, revert = TRUE)
@ 

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{FIML Example}

  Just to make sure our results are plausible, we can do the same analysis using 
  the \src{cfa()} function from the \pkg{lavaan} package:

<<>>=
## Define the model in lavaan syntax:
mod <- "
bmi ~~ ldl + glu
ldl ~~ glu
"

## Fit the model with lavaan::cfa():
fit <- cfa(mod, data = dat2, missing = "fiml")

## Extract the estimated parameters:
muHat2    <- inspect(fit, "est")$nu
sigmaHat2 <- inspect(fit, "theta")
@

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{FIML Example}
  
<<echo = FALSE, results = "asis">>=
muMat           <- rbind(muHat1, t(muHat2))
colnames(muMat) <- colnames(dat1)
rownames(muMat) <- c("Manual", "Lavaan")

print(xtable(muMat, digits = 3, caption = "Estimated Means"), booktabs = TRUE)
@ 

\vx{-12}

\begin{columns}
  \begin{column}{0.5\textwidth}
    
<<echo = FALSE, results = "asis">>= 
colnames(sigmaHat1) <- rownames(sigmaHat1) <- colnames(dat1)
print(xtable(sigmaHat1, 
             digits  = 3, 
             caption = "Manual FIML Covariance Matrix"), 
      booktabs = TRUE)
@ 

\end{column}
\begin{column}{0.5\textwidth}
  
<<echo = FALSE, results = "asis">>= 
print(xtable(sigmaHat2, 
             digits  = 3, 
             caption = "Lavaan FIML Covariance Matrix"), 
      booktabs = TRUE)
@

\end{column}
\end{columns}

\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[fragile]{Example}

<<eval = FALSE>>=
ccOut <- cfa(cfaMod, data = bfi, std.lv = TRUE)
fimlOut <- cfa(cfaMod, data = bfi, std.lv = TRUE, missing = "fiml")

fimlOut2 <- bfi %>% 
    mutate(male = as.numeric(sex == "male")) %>%
    cfa.auxiliary(cfaMod, data = ., aux = c("age", "male"), std.lv = TRUE)

compOut <- cfa(cfaMod, data = bfi0$complete, std.lv = TRUE)

summary(ccOut)
summary(fimlOut)
summary(fimlOut2)
summary(miOut)
summary(compOut)
@   
  
\end{frame}

\watermarkon %-----------------------------------------------------------------%

\sectionslide{Auxiliary Variables}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Satisfying the MAR Assumption}
 
  Like MI, FIML also requires MAR data.
  \begin{itemize}
  \item Parameters will be biased with MAR is violated.
  \end{itemize}
  \vb
  Unlike MI, FIML directly treats the missing data while estimating the analysis 
  model.
  \begin{itemize}
  \item The MAR predictors must be included in the analysis model.
  \item Otherwise, FIML reduces to pairwise deletion.
  \end{itemize}
  \vb  
  If the MAR predictors are not substantively interesting variables, naively 
  included them in the analysis model can change the model's meaning.
  
  %\vc
  %\begin{itemize}
  %\item Suppose we are interested in the effect of age on blood pressure.
  %  \begin{itemize}
  %  \item $Y_{BP} = \beta_0 + \beta_1 X_{age} + \varepsilon$
  %    \vc
  %  \item In this model, $\beta_1$ represents the effect of age on blood pressure.
  %  \end{itemize}
  %  \vc
  %\item If missingness in blood pressure depends on blood glucose and total 
  %  cholesterol, we need to add those variables into our model.
  %  \begin{itemize}
  %  \item $Y_{BP} = \beta_0 + \beta_1 X_{age} + \beta_2 X_{glu} + \beta_3 X_{TC} + \varepsilon$
  %    \vc
  %  \item Now, $\beta_1$ represents the partial effect of age, after controlling 
  %    for blood glucose and total cholesterol.
  %  \end{itemize}
  %\end{itemize}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}{Saturated Correlates Technique}

  \citet{graham:2003} developed the \emph{saturated correlates} approach to 
  meet two desiderata:
  \vc
  \begin{enumerate}
  \item Satisfy the MAR assumption by incorporating MAR predictors into the 
    analysis model.
    \vc
  \item Do not affect the fit or substantive meaning of the analysis model.
  \end{enumerate}
  \vb
  The approach entails incorporating the MAR predictors via a fully-saturated 
  covariance structure:
  \vc
  \begin{enumerate}
  \item Allow all MAR predictors to co-vary with all other MAR predictors.
    \vc
  \item Allow all MAR predictors to co-vary with all observed variables in the
    analysis model (or their residuals).
  \end{enumerate}
  
\end{frame}

%------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{References}

  \bibliographystyle{apacite}
  \bibliography{../../bibtex/winter_school_refs.bib}

\end{frame}

%------------------------------------------------------------------------------%

\end{document}
